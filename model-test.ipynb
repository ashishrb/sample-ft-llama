{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d33d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (8.35.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/azureuser/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/azureuser/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b5c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LLAMA-3.1 CORPORATE ASSISTANT EVALUATION\n",
      "============================================================\n",
      "✅ Fine-tuned model found at: /home/azureuser/cloudfiles/code/Users/746582/llama-8b-ft-11th-june/llama-3.1-8b-corporate-assistant-final\n",
      "🔄 Loading tokenizer...\n",
      "✅ Tokenizer loaded successfully\n",
      "🔄 Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6623b0ae8d114d4c830cdd62e7c993a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base model loaded\n",
      "🔄 Loading fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8fe02b960346819673e3183a73e431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuned model with adapters loaded\n",
      "🔄 Creating inference pipelines...\n",
      "✅ Inference pipelines ready\n",
      "\n",
      "🚀 Ready for evaluation!\n",
      "🔋 GPU: NVIDIA H100 NVL\n",
      "📊 GPU Memory: 99.9 GB\n"
     ]
    }
   ],
   "source": [
    "# Load Models for Interactive Testing\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"🤖 LOADING MODELS FOR INTERACTIVE TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model configurations\n",
    "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "FINE_TUNED_MODEL = \"/home/azureuser/cloudfiles/code/Users/746582/llama-8b-ft-11th-june/llama-3.1-8b-corporate-assistant-final\"\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Check if fine-tuned model exists\n",
    "print(f\"🔍 Checking model path: {FINE_TUNED_MODEL}\")\n",
    "if not Path(FINE_TUNED_MODEL).exists():\n",
    "    print(f\"❌ Fine-tuned model not found!\")\n",
    "    print(\"📁 Available directories:\")\n",
    "    parent_dir = Path(FINE_TUNED_MODEL).parent\n",
    "    for item in parent_dir.iterdir():\n",
    "        if item.is_dir() and any(keyword in item.name.lower() for keyword in ['llama', 'corporate', 'assistant', 'final']):\n",
    "            print(f\"   📁 {item.name}\")\n",
    "    \n",
    "    # Try to find the correct path\n",
    "    possible_paths = [\n",
    "        \"/home/azureuser/cloudfiles/code/Users/746582/llama-8b-ft-11th-june/llama-3.1-8b-corporate-assistant\",\n",
    "        \"./llama-3.1-8b-corporate-assistant-final\", \n",
    "        \"./llama-3.1-8b-corporate-assistant\"\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if Path(path).exists():\n",
    "            FINE_TUNED_MODEL = path\n",
    "            print(f\"✅ Found model at: {FINE_TUNED_MODEL}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Cannot find fine-tuned model. Please check the path.\")\n",
    "\n",
    "print(f\"✅ Using fine-tuned model: {FINE_TUNED_MODEL}\")\n",
    "\n",
    "# GPU check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🔋 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"📊 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    torch.cuda.empty_cache()  # Clear memory\n",
    "else:\n",
    "    print(\"⚠️  Running on CPU\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\n🔄 Loading tokenizer...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        token=HF_TOKEN,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"✅ Tokenizer loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "qlora_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "print(\"\\n🔄 Loading base model...\")\n",
    "try:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=qlora_config,\n",
    "        device_map=\"auto\",\n",
    "        token=HF_TOKEN,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    print(\"✅ Base model loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading base model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load fine-tuned model (base + adapters)\n",
    "print(\"\\n🔄 Loading fine-tuned model...\")\n",
    "try:\n",
    "    # Load base model for fine-tuned version\n",
    "    fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=qlora_config,\n",
    "        device_map=\"auto\", \n",
    "        token=HF_TOKEN,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load PEFT adapters\n",
    "    fine_tuned_model = PeftModel.from_pretrained(fine_tuned_model, FINE_TUNED_MODEL)\n",
    "    print(\"✅ Fine-tuned model with adapters loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading fine-tuned model: {e}\")\n",
    "    print(\"🔧 Trying alternative loading method...\")\n",
    "    \n",
    "    try:\n",
    "        # Alternative: Try loading without quantization first\n",
    "        fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            token=HF_TOKEN,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        fine_tuned_model = PeftModel.from_pretrained(fine_tuned_model, FINE_TUNED_MODEL)\n",
    "        print(\"✅ Fine-tuned model loaded (alternative method)\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Alternative loading failed: {e2}\")\n",
    "        print(\"🔧 Will use base model only for testing\")\n",
    "        fine_tuned_model = base_model\n",
    "\n",
    "# Create pipelines\n",
    "print(\"\\n🔄 Creating inference pipelines...\")\n",
    "try:\n",
    "    base_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"✅ Base pipeline created\")\n",
    "    \n",
    "    fine_tuned_pipeline = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=fine_tuned_model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"✅ Fine-tuned pipeline created\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating pipelines: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test pipelines with a simple generation\n",
    "print(\"\\n🧪 Testing pipelines...\")\n",
    "test_prompt = \"Hello\"\n",
    "\n",
    "try:\n",
    "    # Test base pipeline\n",
    "    base_test = base_pipeline(test_prompt, max_new_tokens=5, do_sample=False)\n",
    "    print(\"✅ Base pipeline working\")\n",
    "    \n",
    "    # Test fine-tuned pipeline  \n",
    "    ft_test = fine_tuned_pipeline(test_prompt, max_new_tokens=5, do_sample=False)\n",
    "    print(\"✅ Fine-tuned pipeline working\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Pipeline test warning: {e}\")\n",
    "    print(\"Pipelines may still work for longer generations\")\n",
    "\n",
    "# Memory check\n",
    "if torch.cuda.is_available():\n",
    "    memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\n🔋 GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB used ({memory_used/memory_total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 MODELS LOADED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Variables available:\")\n",
    "print(\"   • base_pipeline - Base Llama-3.1 model\")\n",
    "print(\"   • fine_tuned_pipeline - Your fine-tuned model\") \n",
    "print(\"   • tokenizer - Tokenizer for both models\")\n",
    "print(\"\\n🚀 Ready for interactive testing!\")\n",
    "print(\"📝 Now run the interactive testing interface cells\")\n",
    "\n",
    "# Quick validation\n",
    "print(\"\\n🔍 QUICK VALIDATION:\")\n",
    "print(f\"✅ base_pipeline: {type(base_pipeline)}\")\n",
    "print(f\"✅ fine_tuned_pipeline: {type(fine_tuned_pipeline)}\")\n",
    "print(f\"✅ tokenizer: {type(tokenizer)}\")\n",
    "\n",
    "print(\"\\n💡 Next steps:\")\n",
    "print(\"1. Run the interactive testing interface cell\")\n",
    "print(\"2. Start testing your model with custom questions!\")\n",
    "print(\"3. Compare base vs fine-tuned responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97313758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up interactive testing interface...\n",
      "⚠️  Models not found. Please run the model loading cells first!\n",
      "Required variables: base_pipeline, fine_tuned_pipeline, tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Interactive Model Testing Interface\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Create interface components\n",
    "class ModelTestingInterface:\n",
    "    def __init__(self, base_pipeline, fine_tuned_pipeline, tokenizer):\n",
    "        self.base_pipeline = base_pipeline\n",
    "        self.fine_tuned_pipeline = fine_tuned_pipeline\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_history = []\n",
    "        \n",
    "        # Create widgets\n",
    "        self.create_widgets()\n",
    "        self.setup_interface()\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create all interface widgets\"\"\"\n",
    "        \n",
    "        # Header\n",
    "        self.header = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); \n",
    "                        padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "                <h2 style='color: white; text-align: center; margin: 0;'>\n",
    "                    🤖 Llama-3.1 Corporate Assistant Testing Interface\n",
    "                </h2>\n",
    "                <p style='color: white; text-align: center; margin: 5px 0 0 0;'>\n",
    "                    Test your fine-tuned model with custom questions\n",
    "                </p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Question input\n",
    "        self.question_input = widgets.Textarea(\n",
    "            value='How to raise a staffing SO request?',\n",
    "            placeholder='Enter your corporate question here...',\n",
    "            description='Question:',\n",
    "            layout=widgets.Layout(width='100%', height='80px'),\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Model selection\n",
    "        self.model_selector = widgets.RadioButtons(\n",
    "            options=['Both Models', 'Fine-tuned Only', 'Base Model Only'],\n",
    "            value='Both Models',\n",
    "            description='Compare:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Generation parameters\n",
    "        self.max_tokens = widgets.IntSlider(\n",
    "            value=150,\n",
    "            min=50,\n",
    "            max=300,\n",
    "            step=25,\n",
    "            description='Max Tokens:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.temperature = widgets.FloatSlider(\n",
    "            value=0.7,\n",
    "            min=0.1,\n",
    "            max=1.0,\n",
    "            step=0.1,\n",
    "            description='Temperature:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.top_p = widgets.FloatSlider(\n",
    "            value=0.9,\n",
    "            min=0.1,\n",
    "            max=1.0,\n",
    "            step=0.1,\n",
    "            description='Top P:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Buttons\n",
    "        self.generate_btn = widgets.Button(\n",
    "            description='🚀 Generate Response',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        \n",
    "        self.clear_btn = widgets.Button(\n",
    "            description='🧹 Clear Results',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='150px', height='40px')\n",
    "        )\n",
    "        \n",
    "        self.history_btn = widgets.Button(\n",
    "            description='📋 Show History',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='150px', height='40px')\n",
    "        )\n",
    "        \n",
    "        # Results output\n",
    "        self.results_output = widgets.Output(\n",
    "            layout=widgets.Layout(width='100%', height='600px', border='1px solid #ddd')\n",
    "        )\n",
    "        \n",
    "        # Status\n",
    "        self.status = widgets.HTML(\n",
    "            value=\"<p style='color: green;'>✅ Ready to generate responses</p>\"\n",
    "        )\n",
    "        \n",
    "        # Quick questions\n",
    "        self.quick_questions = [\n",
    "            \"How to raise a staffing SO request?\",\n",
    "            \"What is the difference between PM and FC job codes?\", \n",
    "            \"How to create a CWR SO?\",\n",
    "            \"Process to convert CWR to FTE associates\",\n",
    "            \"Unable to select subcontractor in the system\",\n",
    "            \"Are there any migration benefits available in Google Cloud?\",\n",
    "            \"How to view hardcopy of I-140 approval notice?\",\n",
    "            \"What equipment do I need for a telemedicine appointment?\",\n",
    "            \"Do we still need to validate SOs through email for GGM SOs?\",\n",
    "            \"Unable to create opportunity id in winzone\"\n",
    "        ]\n",
    "        \n",
    "        self.quick_selector = widgets.Dropdown(\n",
    "            options=['Select a sample question...'] + self.quick_questions,\n",
    "            value='Select a sample question...',\n",
    "            description='Quick Test:',\n",
    "            layout=widgets.Layout(width='100%'),\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "    \n",
    "    def setup_interface(self):\n",
    "        \"\"\"Setup event handlers\"\"\"\n",
    "        self.generate_btn.on_click(self.generate_responses)\n",
    "        self.clear_btn.on_click(self.clear_results)\n",
    "        self.history_btn.on_click(self.show_history)\n",
    "        self.quick_selector.observe(self.load_quick_question, names='value')\n",
    "    \n",
    "    def load_quick_question(self, change):\n",
    "        \"\"\"Load selected quick question\"\"\"\n",
    "        if change['new'] != 'Select a sample question...':\n",
    "            self.question_input.value = change['new']\n",
    "    \n",
    "    def generate_response_single(self, pipeline, question, model_name):\n",
    "        \"\"\"Generate response from a single model\"\"\"\n",
    "        system_prompt = \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\"\n",
    "        \n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=self.max_tokens.value,\n",
    "                do_sample=True,\n",
    "                temperature=self.temperature.value,\n",
    "                top_p=self.top_p.value,\n",
    "                repetition_penalty=1.1,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            generated_text = response[0]['generated_text'].strip()\n",
    "            \n",
    "            return {\n",
    "                'text': generated_text,\n",
    "                'time': generation_time,\n",
    "                'tokens': len(generated_text.split()),\n",
    "                'chars': len(generated_text)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': f\"Error generating response: {str(e)}\",\n",
    "                'time': 0,\n",
    "                'tokens': 0,\n",
    "                'chars': 0\n",
    "            }\n",
    "    \n",
    "    def generate_responses(self, button):\n",
    "        \"\"\"Generate responses based on selected models\"\"\"\n",
    "        question = self.question_input.value.strip()\n",
    "        \n",
    "        if not question:\n",
    "            with self.results_output:\n",
    "                print(\"❌ Please enter a question first!\")\n",
    "            return\n",
    "        \n",
    "        # Update status\n",
    "        self.status.value = \"<p style='color: orange;'>⏳ Generating responses...</p>\"\n",
    "        \n",
    "        with self.results_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Header\n",
    "            print(\"🤖 MODEL RESPONSE COMPARISON\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"❓ Question: {question}\")\n",
    "            print(f\"⚙️  Parameters: Max Tokens={self.max_tokens.value}, Temperature={self.temperature.value}, Top-P={self.top_p.value}\")\n",
    "            print(f\"🕐 Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Generate responses based on selection\n",
    "            model_choice = self.model_selector.value\n",
    "            \n",
    "            if model_choice in ['Both Models', 'Base Model Only']:\n",
    "                print(\"\\n🔵 BASE MODEL RESPONSE:\")\n",
    "                print(\"-\" * 50)\n",
    "                base_result = self.generate_response_single(self.base_pipeline, question, \"Base\")\n",
    "                print(base_result['text'])\n",
    "                print(\"-\" * 50)\n",
    "                print(f\"📊 Stats: {base_result['tokens']} tokens, {base_result['chars']} chars, {base_result['time']:.2f}s\")\n",
    "            \n",
    "            if model_choice in ['Both Models', 'Fine-tuned Only']:\n",
    "                print(\"\\n🚀 FINE-TUNED MODEL RESPONSE:\")\n",
    "                print(\"-\" * 50)\n",
    "                ft_result = self.generate_response_single(self.fine_tuned_pipeline, question, \"Fine-tuned\")\n",
    "                print(ft_result['text'])\n",
    "                print(\"-\" * 50)\n",
    "                print(f\"📊 Stats: {ft_result['tokens']} tokens, {ft_result['chars']} chars, {ft_result['time']:.2f}s\")\n",
    "            \n",
    "            # Comparison analysis (if both models)\n",
    "            if model_choice == 'Both Models':\n",
    "                print(\"\\n🔍 COMPARISON ANALYSIS:\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                # Length comparison\n",
    "                base_len = base_result['tokens']\n",
    "                ft_len = ft_result['tokens']\n",
    "                len_diff = ft_len - base_len\n",
    "                \n",
    "                print(f\"📏 Length: Base={base_len} tokens, Fine-tuned={ft_len} tokens ({len_diff:+d})\")\n",
    "                \n",
    "                # Speed comparison\n",
    "                base_speed = base_result['tokens'] / max(base_result['time'], 0.01)\n",
    "                ft_speed = ft_result['tokens'] / max(ft_result['time'], 0.01)\n",
    "                \n",
    "                print(f\"⚡ Speed: Base={base_speed:.1f} tok/s, Fine-tuned={ft_speed:.1f} tok/s\")\n",
    "                \n",
    "                # Simple keyword analysis\n",
    "                corporate_keywords = ['cognizant', 'oneC', 'so', 'request', 'process', 'system', 'application', 'email']\n",
    "                base_keywords = sum(1 for kw in corporate_keywords if kw.lower() in base_result['text'].lower())\n",
    "                ft_keywords = sum(1 for kw in corporate_keywords if kw.lower() in ft_result['text'].lower())\n",
    "                \n",
    "                print(f\"🏢 Corporate Terms: Base={base_keywords}, Fine-tuned={ft_keywords}\")\n",
    "                \n",
    "                if ft_len > base_len and ft_keywords >= base_keywords:\n",
    "                    print(\"✅ Fine-tuned model provided more detailed corporate response\")\n",
    "                elif ft_keywords > base_keywords:\n",
    "                    print(\"✅ Fine-tuned model used more corporate terminology\")\n",
    "                elif base_len > ft_len:\n",
    "                    print(\"⚠️  Base model provided longer response\")\n",
    "                else:\n",
    "                    print(\"➖ Responses are similar in characteristics\")\n",
    "        \n",
    "        # Save to history\n",
    "        history_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'question': question,\n",
    "            'model_choice': model_choice,\n",
    "            'parameters': {\n",
    "                'max_tokens': self.max_tokens.value,\n",
    "                'temperature': self.temperature.value,\n",
    "                'top_p': self.top_p.value\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if model_choice in ['Both Models', 'Base Model Only']:\n",
    "            history_entry['base_response'] = base_result['text']\n",
    "        if model_choice in ['Both Models', 'Fine-tuned Only']:\n",
    "            history_entry['ft_response'] = ft_result['text']\n",
    "            \n",
    "        self.test_history.append(history_entry)\n",
    "        \n",
    "        # Update status\n",
    "        self.status.value = f\"<p style='color: green;'>✅ Responses generated! (Test #{len(self.test_history)})</p>\"\n",
    "    \n",
    "    def clear_results(self, button):\n",
    "        \"\"\"Clear the results output\"\"\"\n",
    "        with self.results_output:\n",
    "            clear_output()\n",
    "            print(\"🧹 Results cleared. Ready for new questions!\")\n",
    "        self.status.value = \"<p style='color: green;'>✅ Ready to generate responses</p>\"\n",
    "    \n",
    "    def show_history(self, button):\n",
    "        \"\"\"Show testing history\"\"\"\n",
    "        with self.results_output:\n",
    "            clear_output()\n",
    "            \n",
    "            if not self.test_history:\n",
    "                print(\"📋 No testing history yet. Generate some responses first!\")\n",
    "                return\n",
    "            \n",
    "            print(f\"📋 TESTING HISTORY ({len(self.test_history)} tests)\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            for i, entry in enumerate(self.test_history[-10:], 1):  # Show last 10\n",
    "                print(f\"\\n🔢 Test #{len(self.test_history)-10+i}:\")\n",
    "                print(f\"🕐 {entry['timestamp'].strftime('%H:%M:%S')}\")\n",
    "                print(f\"❓ {entry['question'][:60]}{'...' if len(entry['question']) > 60 else ''}\")\n",
    "                print(f\"🎛️  {entry['model_choice']} | Tokens: {entry['parameters']['max_tokens']} | Temp: {entry['parameters']['temperature']}\")\n",
    "                print(\"-\" * 40)\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete interface\"\"\"\n",
    "        \n",
    "        # Parameter controls\n",
    "        params_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>🎛️ Generation Parameters</h3>\"),\n",
    "            widgets.HBox([self.max_tokens, self.temperature, self.top_p])\n",
    "        ])\n",
    "        \n",
    "        # Control buttons\n",
    "        buttons_box = widgets.HBox([\n",
    "            self.generate_btn,\n",
    "            self.clear_btn, \n",
    "            self.history_btn\n",
    "        ], layout=widgets.Layout(justify_content='space-between'))\n",
    "        \n",
    "        # Main interface\n",
    "        interface = widgets.VBox([\n",
    "            self.header,\n",
    "            widgets.HTML(\"<h3>📝 Question Input</h3>\"),\n",
    "            self.quick_selector,\n",
    "            self.question_input,\n",
    "            widgets.HTML(\"<h3>🤖 Model Selection</h3>\"),\n",
    "            self.model_selector,\n",
    "            params_box,\n",
    "            widgets.HTML(\"<h3>🚀 Generate & Control</h3>\"),\n",
    "            buttons_box,\n",
    "            self.status,\n",
    "            widgets.HTML(\"<h3>📊 Results</h3>\"),\n",
    "            self.results_output\n",
    "        ])\n",
    "        \n",
    "        return interface\n",
    "\n",
    "# Initialize the interface (assuming models are already loaded)\n",
    "print(\"🔧 Setting up interactive testing interface...\")\n",
    "\n",
    "# Check if models are loaded\n",
    "if 'base_pipeline' not in locals() or 'fine_tuned_pipeline' not in locals():\n",
    "    print(\"⚠️  Models not found. Please run the model loading cells first!\")\n",
    "    print(\"Required variables: base_pipeline, fine_tuned_pipeline, tokenizer\")\n",
    "else:\n",
    "    # Create and display interface\n",
    "    testing_interface = ModelTestingInterface(base_pipeline, fine_tuned_pipeline, tokenizer)\n",
    "    interface_widget = testing_interface.display()\n",
    "    \n",
    "    print(\"✅ Interactive interface ready!\")\n",
    "    print(\"\\n💡 TIPS:\")\n",
    "    print(\"• Use the dropdown to select sample questions\")\n",
    "    print(\"• Adjust parameters to see how they affect responses\")\n",
    "    print(\"• Compare both models to see fine-tuning improvements\")\n",
    "    print(\"• Check history to review previous tests\")\n",
    "    print(\"\\n🚀 Interface loading below...\")\n",
    "    \n",
    "    display(interface_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344671a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up interactive testing interface...\n",
      "⚠️  Models not found. Please run the model loading cells first!\n",
      "Required variables: base_pipeline, fine_tuned_pipeline, tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Interactive Model Testing Interface\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Create interface components\n",
    "class ModelTestingInterface:\n",
    "    def __init__(self, base_pipeline, fine_tuned_pipeline, tokenizer):\n",
    "        self.base_pipeline = base_pipeline\n",
    "        self.fine_tuned_pipeline = fine_tuned_pipeline\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_history = []\n",
    "        \n",
    "        # Create widgets\n",
    "        self.create_widgets()\n",
    "        self.setup_interface()\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create all interface widgets\"\"\"\n",
    "        \n",
    "        # Header\n",
    "        self.header = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); \n",
    "                        padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "                <h2 style='color: white; text-align: center; margin: 0;'>\n",
    "                    🤖 Llama-3.1 Corporate Assistant Testing Interface\n",
    "                </h2>\n",
    "                <p style='color: white; text-align: center; margin: 5px 0 0 0;'>\n",
    "                    Test your fine-tuned model with custom questions\n",
    "                </p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Question input\n",
    "        self.question_input = widgets.Textarea(\n",
    "            value='How to raise a staffing SO request?',\n",
    "            placeholder='Enter your corporate question here...',\n",
    "            description='Question:',\n",
    "            layout=widgets.Layout(width='100%', height='80px'),\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Model selection\n",
    "        self.model_selector = widgets.RadioButtons(\n",
    "            options=['Both Models', 'Fine-tuned Only', 'Base Model Only'],\n",
    "            value='Both Models',\n",
    "            description='Compare:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Generation parameters\n",
    "        self.max_tokens = widgets.IntSlider(\n",
    "            value=150,\n",
    "            min=50,\n",
    "            max=300,\n",
    "            step=25,\n",
    "            description='Max Tokens:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.temperature = widgets.FloatSlider(\n",
    "            value=0.7,\n",
    "            min=0.1,\n",
    "            max=1.0,\n",
    "            step=0.1,\n",
    "            description='Temperature:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.top_p = widgets.FloatSlider(\n",
    "            value=0.9,\n",
    "            min=0.1,\n",
    "            max=1.0,\n",
    "            step=0.1,\n",
    "            description='Top P:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Buttons\n",
    "        self.generate_btn = widgets.Button(\n",
    "            description='🚀 Generate Response',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        \n",
    "        self.clear_btn = widgets.Button(\n",
    "            description='🧹 Clear Results',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='150px', height='40px')\n",
    "        )\n",
    "        \n",
    "        self.history_btn = widgets.Button(\n",
    "            description='📋 Show History',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='150px', height='40px')\n",
    "        )\n",
    "        \n",
    "        # Results output\n",
    "        self.results_output = widgets.Output(\n",
    "            layout=widgets.Layout(width='100%', height='600px', border='1px solid #ddd')\n",
    "        )\n",
    "        \n",
    "        # Status\n",
    "        self.status = widgets.HTML(\n",
    "            value=\"<p style='color: green;'>✅ Ready to generate responses</p>\"\n",
    "        )\n",
    "        \n",
    "        # Quick questions\n",
    "        self.quick_questions = [\n",
    "            \"How to raise a staffing SO request?\",\n",
    "            \"What is the difference between PM and FC job codes?\", \n",
    "            \"How to create a CWR SO?\",\n",
    "            \"Process to convert CWR to FTE associates\",\n",
    "            \"Unable to select subcontractor in the system\",\n",
    "            \"Are there any migration benefits available in Google Cloud?\",\n",
    "            \"How to view hardcopy of I-140 approval notice?\",\n",
    "            \"What equipment do I need for a telemedicine appointment?\",\n",
    "            \"Do we still need to validate SOs through email for GGM SOs?\",\n",
    "            \"Unable to create opportunity id in winzone\"\n",
    "        ]\n",
    "        \n",
    "        self.quick_selector = widgets.Dropdown(\n",
    "            options=['Select a sample question...'] + self.quick_questions,\n",
    "            value='Select a sample question...',\n",
    "            description='Quick Test:',\n",
    "            layout=widgets.Layout(width='100%'),\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "    \n",
    "    def setup_interface(self):\n",
    "        \"\"\"Setup event handlers\"\"\"\n",
    "        self.generate_btn.on_click(self.generate_responses)\n",
    "        self.clear_btn.on_click(self.clear_results)\n",
    "        self.history_btn.on_click(self.show_history)\n",
    "        self.quick_selector.observe(self.load_quick_question, names='value')\n",
    "    \n",
    "    def load_quick_question(self, change):\n",
    "        \"\"\"Load selected quick question\"\"\"\n",
    "        if change['new'] != 'Select a sample question...':\n",
    "            self.question_input.value = change['new']\n",
    "    \n",
    "    def generate_response_single(self, pipeline, question, model_name):\n",
    "        \"\"\"Generate response from a single model\"\"\"\n",
    "        system_prompt = \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\"\n",
    "        \n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=self.max_tokens.value,\n",
    "                do_sample=True,\n",
    "                temperature=self.temperature.value,\n",
    "                top_p=self.top_p.value,\n",
    "                repetition_penalty=1.1,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            generated_text = response[0]['generated_text'].strip()\n",
    "            \n",
    "            return {\n",
    "                'text': generated_text,\n",
    "                'time': generation_time,\n",
    "                'tokens': len(generated_text.split()),\n",
    "                'chars': len(generated_text)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'text': f\"Error generating response: {str(e)}\",\n",
    "                'time': 0,\n",
    "                'tokens': 0,\n",
    "                'chars': 0\n",
    "            }\n",
    "    \n",
    "    def generate_responses(self, button):\n",
    "        \"\"\"Generate responses based on selected models\"\"\"\n",
    "        question = self.question_input.value.strip()\n",
    "        \n",
    "        if not question:\n",
    "            with self.results_output:\n",
    "                print(\"❌ Please enter a question first!\")\n",
    "            return\n",
    "        \n",
    "        # Update status\n",
    "        self.status.value = \"<p style='color: orange;'>⏳ Generating responses...</p>\"\n",
    "        \n",
    "        with self.results_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Header\n",
    "            print(\"🤖 MODEL RESPONSE COMPARISON\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"❓ Question: {question}\")\n",
    "            print(f\"⚙️  Parameters: Max Tokens={self.max_tokens.value}, Temperature={self.temperature.value}, Top-P={self.top_p.value}\")\n",
    "            print(f\"🕐 Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Generate responses based on selection\n",
    "            model_choice = self.model_selector.value\n",
    "            \n",
    "            if model_choice in ['Both Models', 'Base Model Only']:\n",
    "                print(\"\\n🔵 BASE MODEL RESPONSE:\")\n",
    "                print(\"-\" * 50)\n",
    "                base_result = self.generate_response_single(self.base_pipeline, question, \"Base\")\n",
    "                print(base_result['text'])\n",
    "                print(\"-\" * 50)\n",
    "                print(f\"📊 Stats: {base_result['tokens']} tokens, {base_result['chars']} chars, {base_result['time']:.2f}s\")\n",
    "            \n",
    "            if model_choice in ['Both Models', 'Fine-tuned Only']:\n",
    "                print(\"\\n🚀 FINE-TUNED MODEL RESPONSE:\")\n",
    "                print(\"-\" * 50)\n",
    "                ft_result = self.generate_response_single(self.fine_tuned_pipeline, question, \"Fine-tuned\")\n",
    "                print(ft_result['text'])\n",
    "                print(\"-\" * 50)\n",
    "                print(f\"📊 Stats: {ft_result['tokens']} tokens, {ft_result['chars']} chars, {ft_result['time']:.2f}s\")\n",
    "            \n",
    "            # Comparison analysis (if both models)\n",
    "            if model_choice == 'Both Models':\n",
    "                print(\"\\n🔍 COMPARISON ANALYSIS:\")\n",
    "                print(\"-\" * 30)\n",
    "                \n",
    "                # Length comparison\n",
    "                base_len = base_result['tokens']\n",
    "                ft_len = ft_result['tokens']\n",
    "                len_diff = ft_len - base_len\n",
    "                \n",
    "                print(f\"📏 Length: Base={base_len} tokens, Fine-tuned={ft_len} tokens ({len_diff:+d})\")\n",
    "                \n",
    "                # Speed comparison\n",
    "                base_speed = base_result['tokens'] / max(base_result['time'], 0.01)\n",
    "                ft_speed = ft_result['tokens'] / max(ft_result['time'], 0.01)\n",
    "                \n",
    "                print(f\"⚡ Speed: Base={base_speed:.1f} tok/s, Fine-tuned={ft_speed:.1f} tok/s\")\n",
    "                \n",
    "                # Simple keyword analysis\n",
    "                corporate_keywords = ['cognizant', 'oneC', 'so', 'request', 'process', 'system', 'application', 'email']\n",
    "                base_keywords = sum(1 for kw in corporate_keywords if kw.lower() in base_result['text'].lower())\n",
    "                ft_keywords = sum(1 for kw in corporate_keywords if kw.lower() in ft_result['text'].lower())\n",
    "                \n",
    "                print(f\"🏢 Corporate Terms: Base={base_keywords}, Fine-tuned={ft_keywords}\")\n",
    "                \n",
    "                if ft_len > base_len and ft_keywords >= base_keywords:\n",
    "                    print(\"✅ Fine-tuned model provided more detailed corporate response\")\n",
    "                elif ft_keywords > base_keywords:\n",
    "                    print(\"✅ Fine-tuned model used more corporate terminology\")\n",
    "                elif base_len > ft_len:\n",
    "                    print(\"⚠️  Base model provided longer response\")\n",
    "                else:\n",
    "                    print(\"➖ Responses are similar in characteristics\")\n",
    "        \n",
    "        # Save to history\n",
    "        history_entry = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'question': question,\n",
    "            'model_choice': model_choice,\n",
    "            'parameters': {\n",
    "                'max_tokens': self.max_tokens.value,\n",
    "                'temperature': self.temperature.value,\n",
    "                'top_p': self.top_p.value\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if model_choice in ['Both Models', 'Base Model Only']:\n",
    "            history_entry['base_response'] = base_result['text']\n",
    "        if model_choice in ['Both Models', 'Fine-tuned Only']:\n",
    "            history_entry['ft_response'] = ft_result['text']\n",
    "            \n",
    "        self.test_history.append(history_entry)\n",
    "        \n",
    "        # Update status\n",
    "        self.status.value = f\"<p style='color: green;'>✅ Responses generated! (Test #{len(self.test_history)})</p>\"\n",
    "    \n",
    "    def clear_results(self, button):\n",
    "        \"\"\"Clear the results output\"\"\"\n",
    "        with self.results_output:\n",
    "            clear_output()\n",
    "            print(\"🧹 Results cleared. Ready for new questions!\")\n",
    "        self.status.value = \"<p style='color: green;'>✅ Ready to generate responses</p>\"\n",
    "    \n",
    "    def show_history(self, button):\n",
    "        \"\"\"Show testing history\"\"\"\n",
    "        with self.results_output:\n",
    "            clear_output()\n",
    "            \n",
    "            if not self.test_history:\n",
    "                print(\"📋 No testing history yet. Generate some responses first!\")\n",
    "                return\n",
    "            \n",
    "            print(f\"📋 TESTING HISTORY ({len(self.test_history)} tests)\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            for i, entry in enumerate(self.test_history[-10:], 1):  # Show last 10\n",
    "                print(f\"\\n🔢 Test #{len(self.test_history)-10+i}:\")\n",
    "                print(f\"🕐 {entry['timestamp'].strftime('%H:%M:%S')}\")\n",
    "                print(f\"❓ {entry['question'][:60]}{'...' if len(entry['question']) > 60 else ''}\")\n",
    "                print(f\"🎛️  {entry['model_choice']} | Tokens: {entry['parameters']['max_tokens']} | Temp: {entry['parameters']['temperature']}\")\n",
    "                print(\"-\" * 40)\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete interface\"\"\"\n",
    "        \n",
    "        # Parameter controls\n",
    "        params_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>🎛️ Generation Parameters</h3>\"),\n",
    "            widgets.HBox([self.max_tokens, self.temperature, self.top_p])\n",
    "        ])\n",
    "        \n",
    "        # Control buttons\n",
    "        buttons_box = widgets.HBox([\n",
    "            self.generate_btn,\n",
    "            self.clear_btn, \n",
    "            self.history_btn\n",
    "        ], layout=widgets.Layout(justify_content='space-between'))\n",
    "        \n",
    "        # Main interface\n",
    "        interface = widgets.VBox([\n",
    "            self.header,\n",
    "            widgets.HTML(\"<h3>📝 Question Input</h3>\"),\n",
    "            self.quick_selector,\n",
    "            self.question_input,\n",
    "            widgets.HTML(\"<h3>🤖 Model Selection</h3>\"),\n",
    "            self.model_selector,\n",
    "            params_box,\n",
    "            widgets.HTML(\"<h3>🚀 Generate & Control</h3>\"),\n",
    "            buttons_box,\n",
    "            self.status,\n",
    "            widgets.HTML(\"<h3>📊 Results</h3>\"),\n",
    "            self.results_output\n",
    "        ])\n",
    "        \n",
    "        return interface\n",
    "\n",
    "# Initialize the interface (assuming models are already loaded)\n",
    "print(\"🔧 Setting up interactive testing interface...\")\n",
    "\n",
    "# Check if models are loaded\n",
    "if 'base_pipeline' not in locals() or 'fine_tuned_pipeline' not in locals():\n",
    "    print(\"⚠️  Models not found. Please run the model loading cells first!\")\n",
    "    print(\"Required variables: base_pipeline, fine_tuned_pipeline, tokenizer\")\n",
    "else:\n",
    "    # Create and display interface\n",
    "    testing_interface = ModelTestingInterface(base_pipeline, fine_tuned_pipeline, tokenizer)\n",
    "    interface_widget = testing_interface.display()\n",
    "    \n",
    "    print(\"✅ Interactive interface ready!\")\n",
    "    print(\"\\n💡 TIPS:\")\n",
    "    print(\"• Use the dropdown to select sample questions\")\n",
    "    print(\"• Adjust parameters to see how they affect responses\")\n",
    "    print(\"• Compare both models to see fine-tuning improvements\")\n",
    "    print(\"• Check history to review previous tests\")\n",
    "    print(\"\\n🚀 Interface loading below...\")\n",
    "    \n",
    "    display(interface_widget)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
