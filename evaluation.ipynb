{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Load Fine-tuned Model for Evaluation\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ü§ñ LLAMA-3.1 CORPORATE ASSISTANT EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model paths\n",
    "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "FINE_TUNED_MODEL = \"/home/azureuser/cloudfiles/code/Users/746582/llama-8b-ft-11th-june/llama-3.1-8b-corporate-assistant-final\"\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Check if fine-tuned model exists\n",
    "if not Path(FINE_TUNED_MODEL).exists():\n",
    "    print(f\"‚ùå Fine-tuned model not found at: {FINE_TUNED_MODEL}\")\n",
    "    print(\"Available directories:\")\n",
    "    parent_dir = Path(FINE_TUNED_MODEL).parent\n",
    "    for item in parent_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f\"   üìÅ {item.name}\")\n",
    "    raise FileNotFoundError(\"Please check the model path\")\n",
    "\n",
    "print(f\"‚úÖ Fine-tuned model found at: {FINE_TUNED_MODEL}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"üîÑ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded successfully\")\n",
    "\n",
    "# Load base model for comparison\n",
    "print(\"üîÑ Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base model loaded\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "print(\"üîÑ Loading fine-tuned model...\")\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load PEFT adapter\n",
    "fine_tuned_model = PeftModel.from_pretrained(fine_tuned_model, FINE_TUNED_MODEL)\n",
    "print(\"‚úÖ Fine-tuned model with adapters loaded\")\n",
    "\n",
    "# Create pipelines\n",
    "print(\"üîÑ Creating inference pipelines...\")\n",
    "\n",
    "base_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "fine_tuned_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=fine_tuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Inference pipelines ready\")\n",
    "print(\"\\nüöÄ Ready for evaluation!\")\n",
    "\n",
    "# GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üîã GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Test Cases and Evaluation Functions\n",
    "\n",
    "# Corporate assistant test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"category\": \"Staffing & SO\",\n",
    "        \"question\": \"How to raise a staffing SO request?\",\n",
    "        \"expected_keywords\": [\"OneC\", \"Quick SO\", \"New demand\", \"staffing\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Staffing & SO\", \n",
    "        \"question\": \"What is the difference between PM and FC job codes?\",\n",
    "        \"expected_keywords\": [\"Full Time Employee\", \"Full Time Contractor\", \"PM\", \"FC\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"CWR Process\",\n",
    "        \"question\": \"How to create a CWR SO?\",\n",
    "        \"expected_keywords\": [\"CWR\", \"Contractor Workforce Request\", \"Custom service\", \"CW00\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"CWR Process\",\n",
    "        \"question\": \"Process to convert CWR to FTE associates\",\n",
    "        \"expected_keywords\": [\"New Demand\", \"CWR Conversion\", \"FTE\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"System Issues\",\n",
    "        \"question\": \"Unable to select subcontractor in the system\",\n",
    "        \"expected_keywords\": [\"CWR\", \"OneC\", \"Quick SO\", \"grade selection\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Cloud Services\",\n",
    "        \"question\": \"Are there any migration benefits available in Google Cloud?\",\n",
    "        \"expected_keywords\": [\"migration\", \"google cloud\", \"ProcurementITCloud\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Immigration\",\n",
    "        \"question\": \"How to view hardcopy of I-140 approval notice?\",\n",
    "        \"expected_keywords\": [\"I-140\", \"approval notice\", \"USCIS\", \"company-owned\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Healthcare\",\n",
    "        \"question\": \"What equipment do I need for a telemedicine appointment?\",\n",
    "        \"expected_keywords\": [\"camera\", \"microphone\", \"internet\", \"MHC\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"General Process\",\n",
    "        \"question\": \"Do we still need to validate SOs through email for GGM SOs?\",\n",
    "        \"expected_keywords\": [\"GGM\", \"validation\", \"APAC\", \"automated\"]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"System Access\",\n",
    "        \"question\": \"Unable to create opportunity id in winzone\",\n",
    "        \"expected_keywords\": [\"winzone\", \"opportunity\", \"Account Manager\", \"CRM\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "def generate_response(pipeline, question, max_tokens=200):\n",
    "    \"\"\"Generate response using the given pipeline\"\"\"\n",
    "    system_prompt = \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\"\n",
    "    \n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        \n",
    "        # Extract only the generated text\n",
    "        generated_text = response[0]['generated_text']\n",
    "        return generated_text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "def evaluate_response_quality(response, expected_keywords):\n",
    "    \"\"\"Evaluate response quality based on keyword matching and other factors\"\"\"\n",
    "    if not response or \"Error\" in response:\n",
    "        return 0.0\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    # Keyword matching score (0-40%)\n",
    "    keyword_matches = sum(1 for keyword in expected_keywords if keyword.lower() in response_lower)\n",
    "    keyword_score = (keyword_matches / len(expected_keywords)) * 0.4\n",
    "    \n",
    "    # Length appropriateness (0-20%)\n",
    "    response_length = len(response.split())\n",
    "    if 10 <= response_length <= 100:\n",
    "        length_score = 0.2\n",
    "    elif 5 <= response_length <= 150:\n",
    "        length_score = 0.15\n",
    "    else:\n",
    "        length_score = 0.1\n",
    "    \n",
    "    # Specificity indicators (0-20%)\n",
    "    specific_indicators = [\"step\", \"process\", \"follow\", \"contact\", \"email\", \"application\", \"system\"]\n",
    "    specificity_matches = sum(1 for indicator in specific_indicators if indicator in response_lower)\n",
    "    specificity_score = min(specificity_matches / 3, 1.0) * 0.2\n",
    "    \n",
    "    # Corporate terminology (0-20%)\n",
    "    corporate_terms = [\"cognizant\", \"oneC\", \"so\", \"request\", \"process\", \"system\", \"application\"]\n",
    "    corporate_matches = sum(1 for term in corporate_terms if term.lower() in response_lower)\n",
    "    corporate_score = min(corporate_matches / 3, 1.0) * 0.2\n",
    "    \n",
    "    total_score = keyword_score + length_score + specificity_score + corporate_score\n",
    "    return min(total_score, 1.0)\n",
    "\n",
    "print(\"‚úÖ Test cases and evaluation functions loaded\")\n",
    "print(f\"üìä Total test cases: {len(test_cases)}\")\n",
    "print(f\"üìä Categories: {len(set(case['category'] for case in test_cases))}\")\n",
    "\n",
    "# Show test categories\n",
    "categories = {}\n",
    "for case in test_cases:\n",
    "    category = case['category']\n",
    "    if category not in categories:\n",
    "        categories[category] = 0\n",
    "    categories[category] += 1\n",
    "\n",
    "print(\"\\nüìã Test Categories:\")\n",
    "for category, count in categories.items():\n",
    "    print(f\"   ‚Ä¢ {category}: {count} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Run Comprehensive Evaluation\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üöÄ STARTING COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store results\n",
    "evaluation_results = []\n",
    "\n",
    "# Run evaluation\n",
    "print(\"‚è±Ô∏è  This will take a few minutes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i, test_case in enumerate(tqdm(test_cases, desc=\"Evaluating\")):\n",
    "    print(f\"\\nüìù Question {i+1}/{len(test_cases)}: {test_case['question'][:50]}...\")\n",
    "    \n",
    "    # Generate responses from both models\n",
    "    base_response = generate_response(base_pipeline, test_case['question'])\n",
    "    fine_tuned_response = generate_response(fine_tuned_pipeline, test_case['question'])\n",
    "    \n",
    "    # Evaluate responses\n",
    "    base_score = evaluate_response_quality(base_response, test_case['expected_keywords'])\n",
    "    fine_tuned_score = evaluate_response_quality(fine_tuned_response, test_case['expected_keywords'])\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'question_id': i + 1,\n",
    "        'category': test_case['category'],\n",
    "        'question': test_case['question'],\n",
    "        'expected_keywords': test_case['expected_keywords'],\n",
    "        'base_response': base_response,\n",
    "        'fine_tuned_response': fine_tuned_response,\n",
    "        'base_score': base_score,\n",
    "        'fine_tuned_score': fine_tuned_score,\n",
    "        'improvement': fine_tuned_score - base_score,\n",
    "        'improvement_pct': ((fine_tuned_score - base_score) / max(base_score, 0.01)) * 100\n",
    "    }\n",
    "    \n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    # Quick preview\n",
    "    print(f\"   üìä Base Score: {base_score:.3f} | Fine-tuned Score: {fine_tuned_score:.3f} | Improvement: {result['improvement']:+.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "evaluation_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation completed in {evaluation_time:.1f} seconds\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_results = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Calculate overall statistics\n",
    "overall_stats = {\n",
    "    'avg_base_score': df_results['base_score'].mean(),\n",
    "    'avg_fine_tuned_score': df_results['fine_tuned_score'].mean(),\n",
    "    'avg_improvement': df_results['improvement'].mean(),\n",
    "    'avg_improvement_pct': df_results['improvement_pct'].mean(),\n",
    "    'questions_improved': (df_results['improvement'] > 0).sum(),\n",
    "    'questions_degraded': (df_results['improvement'] < 0).sum(),\n",
    "    'questions_same': (df_results['improvement'] == 0).sum(),\n",
    "    'total_questions': len(df_results)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä OVERALL EVALUATION RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Average Base Model Score:      {overall_stats['avg_base_score']:.3f}\")\n",
    "print(f\"Average Fine-tuned Score:      {overall_stats['avg_fine_tuned_score']:.3f}\")\n",
    "print(f\"Average Improvement:           {overall_stats['avg_improvement']:+.3f}\")\n",
    "print(f\"Average Improvement %:         {overall_stats['avg_improvement_pct']:+.1f}%\")\n",
    "print(f\"Questions Improved:            {overall_stats['questions_improved']}/{overall_stats['total_questions']}\")\n",
    "print(f\"Questions Degraded:            {overall_stats['questions_degraded']}/{overall_stats['total_questions']}\")\n",
    "print(f\"Success Rate:                  {(overall_stats['questions_improved']/overall_stats['total_questions'])*100:.1f}%\")\n",
    "\n",
    "# Category-wise analysis\n",
    "print(\"\\nüìà CATEGORY-WISE PERFORMANCE:\")\n",
    "print(\"=\" * 40)\n",
    "category_stats = df_results.groupby('category').agg({\n",
    "    'base_score': 'mean',\n",
    "    'fine_tuned_score': 'mean', \n",
    "    'improvement': 'mean',\n",
    "    'improvement_pct': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "for category in category_stats.index:\n",
    "    stats = category_stats.loc[category]\n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  Base: {stats['base_score']:.3f} ‚Üí Fine-tuned: {stats['fine_tuned_score']:.3f} ({stats['improvement']:+.3f})\")\n",
    "\n",
    "print(f\"\\nüíæ Results stored in 'df_results' DataFrame with {len(df_results)} rows\")\n",
    "print(\"üéØ Ready for visualization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79872e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Comprehensive Visualization Dashboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a comprehensive dashboard\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Overall Performance Comparison (Top Left)\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "models = ['Base Model', 'Fine-tuned Model']\n",
    "scores = [overall_stats['avg_base_score'], overall_stats['avg_fine_tuned_score']]\n",
    "colors = ['#ff7f7f', '#7fbf7f']\n",
    "\n",
    "bars = ax1.bar(models, scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_title('üèÜ Overall Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Average Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add improvement annotation\n",
    "improvement = overall_stats['avg_improvement']\n",
    "ax1.annotate(f'Improvement: +{improvement:.3f}\\n({overall_stats[\"avg_improvement_pct\"]:+.1f}%)',\n",
    "             xy=(1, scores[1]), xytext=(1.3, scores[1]),\n",
    "             arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "             fontsize=11, fontweight='bold', color='green')\n",
    "\n",
    "# 2. Score Distribution (Top Center)\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, df_results['base_score'], width, label='Base Model', \n",
    "                color='#ff7f7f', alpha=0.8)\n",
    "bars2 = ax2.bar(x + width/2, df_results['fine_tuned_score'], width, label='Fine-tuned Model',\n",
    "                color='#7fbf7f', alpha=0.8)\n",
    "\n",
    "ax2.set_title('üìä Score Distribution by Question', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Question ID')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f'Q{i+1}' for i in range(len(df_results))], rotation=45)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# 3. Improvement Heatmap (Top Right)\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "# Create improvement matrix by category\n",
    "category_improvement = df_results.groupby('category')['improvement'].apply(list)\n",
    "max_questions = max(len(improvements) for improvements in category_improvement.values)\n",
    "\n",
    "# Pad arrays and create matrix\n",
    "improvement_matrix = []\n",
    "category_labels = []\n",
    "for category, improvements in category_improvement.items():\n",
    "    padded = improvements + [np.nan] * (max_questions - len(improvements))\n",
    "    improvement_matrix.append(padded)\n",
    "    category_labels.append(category)\n",
    "\n",
    "improvement_matrix = np.array(improvement_matrix)\n",
    "\n",
    "# Create heatmap\n",
    "im = ax3.imshow(improvement_matrix, cmap='RdYlGn', aspect='auto', vmin=-0.3, vmax=0.3)\n",
    "ax3.set_title('üå°Ô∏è Improvement Heatmap by Category', fontsize=14, fontweight='bold')\n",
    "ax3.set_yticks(range(len(category_labels)))\n",
    "ax3.set_yticklabels(category_labels, fontsize=9)\n",
    "ax3.set_xlabel('Question within Category')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax3, shrink=0.8)\n",
    "cbar.set_label('Improvement Score')\n",
    "\n",
    "# 4. Category Performance (Middle Left)\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "category_stats_plot = df_results.groupby('category').agg({\n",
    "    'base_score': 'mean',\n",
    "    'fine_tuned_score': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "categories = category_stats_plot.index\n",
    "x_pos = np.arange(len(categories))\n",
    "\n",
    "bars1 = ax4.bar(x_pos - 0.2, category_stats_plot['base_score'], 0.4, \n",
    "                label='Base Model', color='#ff7f7f', alpha=0.8)\n",
    "bars2 = ax4.bar(x_pos + 0.2, category_stats_plot['fine_tuned_score'], 0.4,\n",
    "                label='Fine-tuned Model', color='#7fbf7f', alpha=0.8)\n",
    "\n",
    "ax4.set_title('üìà Performance by Category', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Category')\n",
    "ax4.set_ylabel('Average Score')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(categories, rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "# 5. Success Rate Pie Chart (Middle Center)\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "success_data = [\n",
    "    overall_stats['questions_improved'],\n",
    "    overall_stats['questions_degraded'], \n",
    "    overall_stats['questions_same']\n",
    "]\n",
    "labels = ['Improved', 'Degraded', 'Same']\n",
    "colors = ['#7fbf7f', '#ff7f7f', '#ffff7f']\n",
    "explode = (0.1, 0, 0)  # explode the \"Improved\" slice\n",
    "\n",
    "wedges, texts, autotexts = ax5.pie(success_data, labels=labels, colors=colors, explode=explode,\n",
    "                                   autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
    "\n",
    "ax5.set_title('üéØ Question Improvement Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 6. Response Length Comparison (Middle Right)\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "base_lengths = [len(resp.split()) for resp in df_results['base_response']]\n",
    "ft_lengths = [len(resp.split()) for resp in df_results['fine_tuned_response']]\n",
    "\n",
    "ax6.hist(base_lengths, bins=15, alpha=0.7, label='Base Model', color='#ff7f7f')\n",
    "ax6.hist(ft_lengths, bins=15, alpha=0.7, label='Fine-tuned Model', color='#7fbf7f')\n",
    "ax6.set_title('üìè Response Length Distribution', fontsize=14, fontweight='bold')\n",
    "ax6.set_xlabel('Response Length (words)')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.legend()\n",
    "\n",
    "# 7. Training Metrics Visualization (Bottom Left)\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "# Load training metrics if available\n",
    "try:\n",
    "    with open(f\"{FINE_TUNED_MODEL}/training_metrics.json\", 'r') as f:\n",
    "        training_logs = json.load(f)\n",
    "    \n",
    "    # Extract training and validation losses\n",
    "    epochs = []\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    \n",
    "    for log in training_logs:\n",
    "        if 'epoch' in log and 'loss' in log:\n",
    "            epochs.append(log['epoch'])\n",
    "            train_losses.append(log['loss'])\n",
    "        if 'epoch' in log and 'eval_loss' in log:\n",
    "            eval_losses.append(log['eval_loss'])\n",
    "    \n",
    "    if epochs and train_losses:\n",
    "        ax7.plot(epochs, train_losses, 'o-', label='Training Loss', color='blue', linewidth=2)\n",
    "        if eval_losses:\n",
    "            eval_epochs = [log['epoch'] for log in training_logs if 'eval_loss' in log]\n",
    "            ax7.plot(eval_epochs, eval_losses, 's-', label='Validation Loss', color='red', linewidth=2)\n",
    "        \n",
    "        ax7.set_title('üìâ Training Progress', fontsize=14, fontweight='bold')\n",
    "        ax7.set_xlabel('Epoch')\n",
    "        ax7.set_ylabel('Loss')\n",
    "        ax7.legend()\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax7.text(0.5, 0.5, 'Training metrics\\nnot available', ha='center', va='center',\n",
    "                transform=ax7.transAxes, fontsize=12)\n",
    "        ax7.set_title('üìâ Training Progress', fontsize=14, fontweight='bold')\n",
    "\n",
    "except:\n",
    "    ax7.text(0.5, 0.5, 'Training metrics\\nnot found', ha='center', va='center',\n",
    "            transform=ax7.transAxes, fontsize=12)\n",
    "    ax7.set_title('üìâ Training Progress', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 8. Top Improvements (Bottom Center)\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "top_improvements = df_results.nlargest(5, 'improvement')[['question_id', 'improvement']]\n",
    "bottom_improvements = df_results.nsmallest(3, 'improvement')[['question_id', 'improvement']]\n",
    "\n",
    "all_changes = pd.concat([top_improvements, bottom_improvements])\n",
    "colors_change = ['green' if x > 0 else 'red' for x in all_changes['improvement']]\n",
    "\n",
    "bars = ax8.barh(range(len(all_changes)), all_changes['improvement'], color=colors_change, alpha=0.7)\n",
    "ax8.set_title('üîÑ Biggest Changes by Question', fontsize=14, fontweight='bold')\n",
    "ax8.set_xlabel('Score Change')\n",
    "ax8.set_yticks(range(len(all_changes)))\n",
    "ax8.set_yticklabels([f\"Q{int(qid)}\" for qid in all_changes['question_id']])\n",
    "ax8.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, value) in enumerate(zip(bars, all_changes['improvement'])):\n",
    "    ax8.text(value + (0.01 if value > 0 else -0.01), i, f'{value:.3f}', \n",
    "             va='center', ha='left' if value > 0 else 'right', fontweight='bold')\n",
    "\n",
    "# 9. Summary Statistics (Bottom Right)\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "ax9.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "üéØ EVALUATION SUMMARY\n",
    "\n",
    "üìä Total Questions: {overall_stats['total_questions']}\n",
    "‚úÖ Questions Improved: {overall_stats['questions_improved']} ({overall_stats['questions_improved']/overall_stats['total_questions']*100:.1f}%)\n",
    "‚ùå Questions Degraded: {overall_stats['questions_degraded']} ({overall_stats['questions_degraded']/overall_stats['total_questions']*100:.1f}%)\n",
    "‚ûñ Questions Same: {overall_stats['questions_same']} ({overall_stats['questions_same']/overall_stats['total_questions']*100:.1f}%)\n",
    "\n",
    "üìà Average Improvement: {overall_stats['avg_improvement']:+.3f}\n",
    "üìà Percentage Improvement: {overall_stats['avg_improvement_pct']:+.1f}%\n",
    "\n",
    "üèÜ Best Category: {category_stats.loc[category_stats['improvement'].idxmax()].name}\n",
    "üîß Needs Work: {category_stats.loc[category_stats['improvement'].idxmin()].name}\n",
    "\n",
    "‚≠ê Overall Assessment: {'EXCELLENT' if overall_stats['avg_improvement'] > 0.1 else 'GOOD' if overall_stats['avg_improvement'] > 0.05 else 'FAIR' if overall_stats['avg_improvement'] > 0 else 'NEEDS IMPROVEMENT'}\n",
    "\"\"\"\n",
    "\n",
    "ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=11,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ü§ñ LLAMA-3.1 CORPORATE ASSISTANT EVALUATION DASHBOARD ü§ñ', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Evaluation dashboard complete!\")\n",
    "print(f\"üíæ Results saved in 'df_results' DataFrame\")\n",
    "print(f\"üìà Overall improvement: {overall_stats['avg_improvement']:+.3f} ({overall_stats['avg_improvement_pct']:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737faa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Interactive Response Comparison\n",
    "\n",
    "def display_comparison(question_id=None, category=None):\n",
    "    \"\"\"Display detailed comparison for specific question or category\"\"\"\n",
    "    \n",
    "    if question_id is not None:\n",
    "        # Show specific question\n",
    "        result = df_results[df_results['question_id'] == question_id].iloc[0]\n",
    "        results_to_show = [result]\n",
    "    elif category is not None:\n",
    "        # Show all questions from category\n",
    "        results_to_show = df_results[df_results['category'] == category].to_dict('records')\n",
    "    else:\n",
    "        # Show top 3 improved questions\n",
    "        results_to_show = df_results.nlargest(3, 'improvement').to_dict('records')\n",
    "    \n",
    "    for i, result in enumerate(results_to_show):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üîç QUESTION {result['question_id']}: {result['category']}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"‚ùì Question: {result['question']}\")\n",
    "        print(f\"üéØ Expected Keywords: {', '.join(result['expected_keywords'])}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ü§ñ BASE MODEL RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result['base_response'])\n",
    "        print(f\"üìä Score: {result['base_score']:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üöÄ FINE-TUNED MODEL RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result['fine_tuned_response'])\n",
    "        print(f\"üìä Score: {result['fine_tuned_score']:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        # Improvement analysis\n",
    "        improvement = result['improvement']\n",
    "        if improvement > 0:\n",
    "            print(f\"‚úÖ IMPROVEMENT: +{improvement:.3f} ({result['improvement_pct']:+.1f}%)\")\n",
    "            print(\"üéâ Fine-tuned model performed better!\")\n",
    "        elif improvement < 0:\n",
    "            print(f\"‚ùå REGRESSION: {improvement:.3f} ({result['improvement_pct']:+.1f}%)\")\n",
    "            print(\"‚ö†Ô∏è  Base model performed better on this question\")\n",
    "        else:\n",
    "            print(\"‚ûñ NO CHANGE: Both models performed equally\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        if i < len(results_to_show) - 1:\n",
    "            print()\n",
    "\n",
    "# Interactive functions\n",
    "def show_best_improvements(n=3):\n",
    "    \"\"\"Show top N improved questions\"\"\"\n",
    "    print(f\"üèÜ TOP {n} IMPROVED QUESTIONS:\")\n",
    "    best = df_results.nlargest(n, 'improvement')\n",
    "    for _, row in best.iterrows():\n",
    "        print(f\"Q{row['question_id']}: +{row['improvement']:.3f} ({row['improvement_pct']:+.1f}%) - {row['question'][:60]}...\")\n",
    "\n",
    "def show_worst_regressions(n=3):\n",
    "    \"\"\"Show top N degraded questions\"\"\"\n",
    "    print(f\"‚ö†Ô∏è  TOP {n} DEGRADED QUESTIONS:\")\n",
    "    worst = df_results.nsmallest(n, 'improvement')\n",
    "    for _, row in worst.iterrows():\n",
    "        print(f\"Q{row['question_id']}: {row['improvement']:.3f} ({row['improvement_pct']:+.1f}%) - {row['question'][:60]}...\")\n",
    "\n",
    "def show_category_performance():\n",
    "    \"\"\"Show performance by category\"\"\"\n",
    "    print(\"üìä PERFORMANCE BY CATEGORY:\")\n",
    "    print(\"=\" * 50)\n",
    "    category_stats = df_results.groupby('category').agg({\n",
    "        'base_score': 'mean',\n",
    "        'fine_tuned_score': 'mean',\n",
    "        'improvement': 'mean',\n",
    "        'improvement_pct': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    for category in category_stats.index:\n",
    "        stats = category_stats.loc[category]\n",
    "        status = \"‚úÖ\" if stats['improvement'] > 0 else \"‚ùå\" if stats['improvement'] < 0 else \"‚ûñ\"\n",
    "        print(f\"{status} {category}:\")\n",
    "        print(f\"   Base: {stats['base_score']:.3f} ‚Üí Fine-tuned: {stats['fine_tuned_score']:.3f}\")\n",
    "        print(f\"   Change: {stats['improvement']:+.3f} ({stats['improvement_pct']:+.1f}%)\")\n",
    "        print()\n",
    "\n",
    "def ask_model_interactive():\n",
    "    \"\"\"Interactive function to test the model with custom questions\"\"\"\n",
    "    print(\"üéÆ INTERACTIVE MODEL TESTING\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Ask your fine-tuned model any corporate question!\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"‚ùì Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Thanks for testing!\")\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        print(\"\\nü§ñ Generating response...\")\n",
    "        \n",
    "        # Get response from fine-tuned model\n",
    "        response = generate_response(fine_tuned_pipeline, question, max_tokens=150)\n",
    "        \n",
    "        print(\"\\nüöÄ FINE-TUNED MODEL RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(response)\n",
    "        print(\"-\" * 40)\n",
    "        print()\n",
    "\n",
    "# Quick access functions\n",
    "print(\"üéØ QUICK EVALUATION FUNCTIONS AVAILABLE:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚Ä¢ show_best_improvements(n=3) - Show top improved questions\")\n",
    "print(\"‚Ä¢ show_worst_regressions(n=3) - Show degraded questions\") \n",
    "print(\"‚Ä¢ show_category_performance() - Performance by category\")\n",
    "print(\"‚Ä¢ display_comparison(question_id=X) - Show specific question\")\n",
    "print(\"‚Ä¢ display_comparison(category='Category Name') - Show category\")\n",
    "print(\"‚Ä¢ ask_model_interactive() - Test with custom questions\")\n",
    "print()\n",
    "\n",
    "# Show some quick insights\n",
    "print(\"üöÄ QUICK INSIGHTS:\")\n",
    "print(\"=\" * 20)\n",
    "show_best_improvements(3)\n",
    "print()\n",
    "show_worst_regressions(2)\n",
    "print()\n",
    "show_category_performance()\n",
    "\n",
    "print(\"\\nüí° TIP: Use display_comparison(question_id=5) to see detailed comparison for question 5\")\n",
    "print(\"üí° TIP: Use ask_model_interactive() to test with your own questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Export Results and Generate Report\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def generate_evaluation_report():\n",
    "    \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_filename = f\"llama_evaluation_report_{timestamp}.md\"\n",
    "    \n",
    "    # Prepare detailed results\n",
    "    best_questions = df_results.nlargest(3, 'improvement')\n",
    "    worst_questions = df_results.nsmallest(3, 'improvement')\n",
    "    \n",
    "    report_content = f\"\"\"# Llama-3.1 Corporate Assistant Evaluation Report\n",
    "\n",
    "**Generated:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**Model:** Llama-3.1-8B-Instruct (Fine-tuned)\n",
    "**Model Path:** {FINE_TUNED_MODEL}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "### Overall Performance\n",
    "- **Total Questions Evaluated:** {overall_stats['total_questions']}\n",
    "- **Average Base Model Score:** {overall_stats['avg_base_score']:.3f}\n",
    "- **Average Fine-tuned Score:** {overall_stats['avg_fine_tuned_score']:.3f}\n",
    "- **Overall Improvement:** {overall_stats['avg_improvement']:+.3f} ({overall_stats['avg_improvement_pct']:+.1f}%)\n",
    "\n",
    "### Success Metrics\n",
    "- **Questions Improved:** {overall_stats['questions_improved']}/{overall_stats['total_questions']} ({overall_stats['questions_improved']/overall_stats['total_questions']*100:.1f}%)\n",
    "- **Questions Degraded:** {overall_stats['questions_degraded']}/{overall_stats['total_questions']} ({overall_stats['questions_degraded']/overall_stats['total_questions']*100:.1f}%)\n",
    "- **Questions Unchanged:** {overall_stats['questions_same']}/{overall_stats['total_questions']} ({overall_stats['questions_same']/overall_stats['total_questions']*100:.1f}%)\n",
    "\n",
    "## Category Performance Analysis\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add category analysis\n",
    "    category_stats = df_results.groupby('category').agg({\n",
    "        'base_score': 'mean',\n",
    "        'fine_tuned_score': 'mean',\n",
    "        'improvement': 'mean',\n",
    "        'improvement_pct': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    for category in category_stats.index:\n",
    "        stats = category_stats.loc[category]\n",
    "        status = \"‚úÖ Improved\" if stats['improvement'] > 0 else \"‚ùå Degraded\" if stats['improvement'] < 0 else \"‚ûñ No Change\"\n",
    "        report_content += f\"\"\"\n",
    "### {category} {status}\n",
    "- **Base Score:** {stats['base_score']:.3f}\n",
    "- **Fine-tuned Score:** {stats['fine_tuned_score']:.3f}\n",
    "- **Improvement:** {stats['improvement']:+.3f} ({stats['improvement_pct']:+.1f}%)\n",
    "\"\"\"\n",
    "    \n",
    "    # Add best performing questions\n",
    "    report_content += f\"\"\"\n",
    "## Top Performing Questions\n",
    "\n",
    "### Best Improvements\n",
    "\"\"\"\n",
    "    for _, row in best_questions.iterrows():\n",
    "        report_content += f\"\"\"\n",
    "**Question {row['question_id']}:** {row['question']}\n",
    "- **Category:** {row['category']}\n",
    "- **Base Score:** {row['base_score']:.3f}\n",
    "- **Fine-tuned Score:** {row['fine_tuned_score']:.3f}\n",
    "- **Improvement:** {row['improvement']:+.3f} ({row['improvement_pct']:+.1f}%)\n",
    "\n",
    "*Base Response:* {row['base_response'][:200]}...\n",
    "\n",
    "*Fine-tuned Response:* {row['fine_tuned_response'][:200]}...\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    # Add worst performing questions\n",
    "    report_content += f\"\"\"\n",
    "### Areas for Improvement\n",
    "\"\"\"\n",
    "    for _, row in worst_questions.iterrows():\n",
    "        report_content += f\"\"\"\n",
    "**Question {row['question_id']}:** {row['question']}\n",
    "- **Category:** {row['category']}\n",
    "- **Base Score:** {row['base_score']:.3f}\n",
    "- **Fine-tuned Score:** {row['fine_tuned_score']:.3f}\n",
    "- **Change:** {row['improvement']:+.3f} ({row['improvement_pct']:+.1f}%)\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    # Add recommendations\n",
    "    avg_improvement = overall_stats['avg_improvement']\n",
    "    success_rate = overall_stats['questions_improved'] / overall_stats['total_questions']\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "## Recommendations\n",
    "\n",
    "### Overall Assessment\n",
    "\"\"\"\n",
    "    \n",
    "    if avg_improvement > 0.1 and success_rate > 0.7:\n",
    "        assessment = \"EXCELLENT\"\n",
    "        recommendations = \"\"\"\n",
    "- ‚úÖ The fine-tuning was highly successful\n",
    "- ‚úÖ Deploy the model to production\n",
    "- ‚úÖ Consider expanding the training dataset for even better performance\n",
    "- ‚úÖ Monitor performance in production and collect feedback\n",
    "\"\"\"\n",
    "    elif avg_improvement > 0.05 and success_rate > 0.6:\n",
    "        assessment = \"GOOD\"\n",
    "        recommendations = \"\"\"\n",
    "- ‚úÖ The fine-tuning was successful\n",
    "- ‚ö†Ô∏è Consider additional training on underperforming categories\n",
    "- ‚úÖ Deploy with monitoring and feedback collection\n",
    "- üîß Review and improve training data quality\n",
    "\"\"\"\n",
    "    elif avg_improvement > 0 and success_rate > 0.5:\n",
    "        assessment = \"FAIR\"\n",
    "        recommendations = \"\"\"\n",
    "- ‚ö†Ô∏è Modest improvement achieved\n",
    "- üîß Review training data quality and quantity\n",
    "- üîß Consider adjusting hyperparameters\n",
    "- üîß Add more diverse training examples\n",
    "- ‚ö†Ô∏è Test thoroughly before production deployment\n",
    "\"\"\"\n",
    "    else:\n",
    "        assessment = \"NEEDS IMPROVEMENT\"\n",
    "        recommendations = \"\"\"\n",
    "- ‚ùå Fine-tuning did not achieve desired results\n",
    "- üîß Review training data quality and alignment with evaluation criteria\n",
    "- üîß Consider different model architecture or training approach\n",
    "- üîß Increase training data quantity and diversity\n",
    "- ‚ùå Do not deploy without significant improvements\n",
    "\"\"\"\n",
    "    \n",
    "    report_content += f\"\"\"\n",
    "**Assessment:** {assessment}\n",
    "\n",
    "{recommendations}\n",
    "\n",
    "### Technical Recommendations\n",
    "- **Best Performing Category:** {category_stats.loc[category_stats['improvement'].idxmax()].name}\n",
    "- **Needs Most Attention:** {category_stats.loc[category_stats['improvement'].idxmin()].name}\n",
    "- **Training Data:** Consider adding more examples for underperforming categories\n",
    "- **Model Architecture:** Current LoRA configuration appears effective\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "### Model Configuration\n",
    "- **Base Model:** {BASE_MODEL}\n",
    "- **Fine-tuning Method:** QLoRA (4-bit quantization + LoRA adapters)\n",
    "- **LoRA Rank:** 64\n",
    "- **Target Modules:** q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n",
    "\n",
    "### Evaluation Methodology\n",
    "- **Evaluation Criteria:** Keyword matching, response length, specificity, corporate terminology\n",
    "- **Test Cases:** {len(test_cases)} questions across {len(set(case['category'] for case in test_cases))} categories\n",
    "- **Scoring Range:** 0.0 - 1.0 (higher is better)\n",
    "\n",
    "---\n",
    "\n",
    "*Report generated automatically by Llama-3.1 Corporate Assistant Evaluation System*\n",
    "\"\"\"\n",
    "    \n",
    "    # Save the report\n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_content)\n",
    "    \n",
    "    print(f\"üìÑ Comprehensive report saved as: {report_filename}\")\n",
    "    return report_filename\n",
    "\n",
    "def export_results_csv():\n",
    "    \"\"\"Export detailed results to CSV\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"llama_evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    # Prepare export data\n",
    "    export_df = df_results.copy()\n",
    "    export_df['timestamp'] = datetime.now()\n",
    "    export_df['model_path'] = FINE_TUNED_MODEL\n",
    "    \n",
    "    export_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"üìä Detailed results exported to: {csv_filename}\")\n",
    "    return csv_filename\n",
    "\n",
    "def save_evaluation_plots():\n",
    "    \"\"\"Save the evaluation dashboard as images\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_filename = f\"llama_evaluation_dashboard_{timestamp}.png\"\n",
    "    \n",
    "    # The plot from Cell 4 should still be active\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    print(f\"üìà Evaluation dashboard saved as: {plot_filename}\")\n",
    "    return plot_filename\n",
    "\n",
    "# Generate all outputs\n",
    "print(\"üìã GENERATING EVALUATION OUTPUTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Generate comprehensive report\n",
    "report_file = generate_evaluation_report()\n",
    "\n",
    "# Export CSV data\n",
    "csv_file = export_results_csv()\n",
    "\n",
    "# Save plots\n",
    "plot_file = save_evaluation_plots()\n",
    "\n",
    "print(\"\\n‚úÖ EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"üìÑ Report: {report_file}\")\n",
    "print(f\"üìä Data: {csv_file}\")\n",
    "print(f\"üìà Plots: {plot_file}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüéØ FINAL ASSESSMENT: \", end=\"\")\n",
    "avg_improvement = overall_stats['avg_improvement']\n",
    "success_rate = overall_stats['questions_improved'] / overall_stats['total_questions']\n",
    "\n",
    "if avg_improvement > 0.1 and success_rate > 0.7:\n",
    "    print(\"üèÜ EXCELLENT - Ready for production!\")\n",
    "elif avg_improvement > 0.05 and success_rate > 0.6:\n",
    "    print(\"‚úÖ GOOD - Deploy with monitoring\")\n",
    "elif avg_improvement > 0 and success_rate > 0.5:\n",
    "    print(\"‚ö†Ô∏è FAIR - Needs improvement\")\n",
    "else:\n",
    "    print(\"‚ùå NEEDS WORK - Significant improvements required\")\n",
    "\n",
    "print(f\"\\nüìä Key Metrics:\")\n",
    "print(f\"   ‚Ä¢ Average improvement: {overall_stats['avg_improvement']:+.3f}\")\n",
    "print(f\"   ‚Ä¢ Success rate: {success_rate*100:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Questions improved: {overall_stats['questions_improved']}/{overall_stats['total_questions']}\")\n",
    "\n",
    "print(f\"\\nüíæ All results stored in 'df_results' variable for further analysis\")\n",
    "print(f\"üéÆ Use ask_model_interactive() to test the model with your own questions!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
