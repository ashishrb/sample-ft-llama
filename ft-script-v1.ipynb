{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe96f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install -q transformers>=4.43.0\n",
    "%pip install -q accelerate>=0.21.0\n",
    "%pip install -q peft>=0.4.0\n",
    "%pip install -q bitsandbytes>=0.41.0\n",
    "%pip install -q trl>=0.7.0\n",
    "%pip install -q datasets\n",
    "%pip install -q scipy\n",
    "%pip install -q tensorboard\n",
    "%pip install -q wandb\n",
    "%pip install -q sentencepiece\n",
    "%pip install -q protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aab9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# FIX 1: Set tokenizer parallelism environment variable BEFORE importing anything\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"Environment configured:\")\n",
    "print(f\"‚úÖ TOKENIZERS_PARALLELISM = {os.environ.get('TOKENIZERS_PARALLELISM')}\")\n",
    "print(\"‚úÖ Warnings suppressed\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup device and check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\" if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622eafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# QLoRA configuration - optimized for H100\n",
    "qlora_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA configuration - balanced for instruction tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False\n",
    ")\n",
    "\n",
    "# Training hyperparameters - optimized for H100 with 40 cores\n",
    "training_config = {\n",
    "    \"output_dir\": \"./llama-3.1-8b-corporate-assistant\",\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"max_steps\": -1,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"group_by_length\": True,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"report_to\": \"tensorboard\",\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 100,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_total_limit\": 1,  # Reduced to avoid conflicts\n",
    "    \"load_best_model_at_end\": False,  # FIX: Prevents 'method' object error\n",
    "    \"dataloader_num_workers\": 2,  # Reduced from 8 to avoid conflicts\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"push_to_hub\": False,\n",
    "    \"seed\": 42,\n",
    "    \"data_seed\": 42,\n",
    "    \"ddp_find_unused_parameters\": False,  # Helps with distributed training\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Training configuration loaded with fixes applied\")\n",
    "print(f\"üìä Total epochs: {training_config['num_train_epochs']}\")\n",
    "print(f\"üìä Effective batch size: {training_config['per_device_train_batch_size'] * training_config['gradient_accumulation_steps']}\")\n",
    "print(f\"üìä Learning rate: {training_config['learning_rate']}\")\n",
    "print(f\"‚ö†Ô∏è  load_best_model_at_end: {training_config['load_best_model_at_end']} (disabled to prevent errors)\")\n",
    "print(f\"‚ö†Ô∏è  dataloader_num_workers: {training_config['dataloader_num_workers']} (reduced to prevent conflicts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3577411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_and_load_dataset(file_path):\n",
    "    \"\"\"Check file exists and load dataset safely\"\"\"\n",
    "    print(f\"üîç Checking file: {file_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        print(\"üìù Available files in directory:\")\n",
    "        parent_dir = path.parent\n",
    "        if parent_dir.exists():\n",
    "            for file in parent_dir.iterdir():\n",
    "                if file.suffix in ['.jsonl', '.json']:\n",
    "                    print(f\"   - {file}\")\n",
    "        return None\n",
    "    \n",
    "    if path.stat().st_size == 0:\n",
    "        print(f\"‚ùå File is empty: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ File found, size: {path.stat().st_size} bytes\")\n",
    "    \n",
    "    # Load data\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    # Validate entry\n",
    "                    if isinstance(entry, dict) and 'instruction' in entry and 'output' in entry:\n",
    "                        # Ensure system message exists\n",
    "                        if 'system' not in entry:\n",
    "                            entry['system'] = \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\"\n",
    "                        data.append(entry)\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Skipping invalid entry at line {line_num}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"‚ö†Ô∏è  Invalid JSON at line {line_num}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(data)} valid entries\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa776acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_jsonl_data(file_path):\n",
    "#     \"\"\"Load data from JSONL file\"\"\"\n",
    "#     data = []\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             data.append(json.loads(line.strip()))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f657345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conversational_format(data_entry):\n",
    "    \"\"\"Convert system/instruction/output format to conversational format\"\"\"\n",
    "    system_message = data_entry.get('system', 'You are a helpful assistant.')\n",
    "    instruction = data_entry.get('instruction', '')\n",
    "    output = data_entry.get('output', '')\n",
    "    \n",
    "    conversation = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_safe(file_path, test_size=0.1):\n",
    "    \"\"\"Load and preprocess the dataset with error handling\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"DATASET LOADING AND PREPROCESSING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Try to load the dataset\n",
    "    raw_data = check_file_and_load_dataset(file_path)\n",
    "    \n",
    "    if raw_data is None or len(raw_data) == 0:\n",
    "        print(\"‚ùå No valid data found. Please check your dataset file.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to conversational format\n",
    "    print(\"üîÑ Converting to conversational format...\")\n",
    "    conversations = []\n",
    "    \n",
    "    for i, entry in enumerate(raw_data):\n",
    "        try:\n",
    "            conv = convert_to_conversational_format(entry)\n",
    "            conversations.append({\"text\": conv})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error processing entry {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(conversations) == 0:\n",
    "        print(\"‚ùå No conversations could be created\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(conversations)} conversations\")\n",
    "    \n",
    "    # Create dataset\n",
    "    try:\n",
    "        dataset = Dataset.from_list(conversations)\n",
    "        \n",
    "        # Split into train/validation\n",
    "        if len(conversations) > 1:\n",
    "            dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "        else:\n",
    "            # Single example case\n",
    "            dataset = DatasetDict({\n",
    "                'train': dataset,\n",
    "                'test': dataset.select([0])\n",
    "            })\n",
    "        \n",
    "        print(f\"üìä Train examples: {len(dataset['train'])}\")\n",
    "        print(f\"üìä Validation examples: {len(dataset['test'])}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\n=== SAMPLE CONVERSATION ===\")\n",
    "        sample_text = dataset['train'][0]['text']\n",
    "        print(sample_text[:300] + \"...\" if len(sample_text) > 300 else sample_text)\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Update your dataset path here\n",
    "DATASET_PATH = \"/home/azureuser/cloudfiles/code/Users/746582/llama-8b-ft-11th-june/azure_instruction_dataset.jsonl\"\n",
    "\n",
    "# Load dataset with error handling\n",
    "dataset = preprocess_dataset_safe(DATASET_PATH)\n",
    "\n",
    "# Check if dataset loaded successfully\n",
    "if dataset is None:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DATASET LOADING FAILED!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. File path is correct\")\n",
    "    print(\"2. File exists and is not empty\") \n",
    "    print(\"3. File contains valid JSONL format\")\n",
    "    print(\"4. Each line has 'instruction' and 'output' fields\")\n",
    "    raise ValueError(\"Dataset loading failed. Cannot proceed with training.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"DATASET LOADED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"‚úÖ Ready to proceed with model loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_dataset(file_path, test_size=0.1):\n",
    "#     \"\"\"Load and preprocess the dataset\"\"\"\n",
    "#     print(\"Loading dataset...\")\n",
    "#     raw_data = load_jsonl_data(file_path)\n",
    "#     print(f\"Loaded {len(raw_data)} examples\")\n",
    "    \n",
    "#     # Convert to conversational format\n",
    "#     print(\"Converting to conversational format...\")\n",
    "#     conversations = []\n",
    "#     for entry in raw_data:\n",
    "#         conv = convert_to_conversational_format(entry)\n",
    "#         conversations.append({\"text\": conv})\n",
    "    \n",
    "#     # Create dataset\n",
    "#     dataset = Dataset.from_list(conversations)\n",
    "    \n",
    "#     # Split into train/validation\n",
    "#     dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "    \n",
    "#     print(f\"Train examples: {len(dataset['train'])}\")\n",
    "#     print(f\"Validation examples: {len(dataset['test'])}\")\n",
    "    \n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (replace with your file path)\n",
    "DATASET_PATH = \"/home/azureuser/cloudfiles/code/Users/746582/llama-8b-ft-11th-june/azure_instruction_dataset.jsonl\"  # Update this path\n",
    "dataset = preprocess_dataset_safe(DATASET_PATH)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n=== Sample Conversation ===\")\n",
    "print(dataset['train'][0]['text'][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model with QLoRA...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=qlora_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}\")\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèãÔ∏è Creating trainer with error handling...\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(**training_config)\n",
    "\n",
    "# Verify dataset is loaded\n",
    "if 'dataset' not in locals() or dataset is None:\n",
    "    raise ValueError(\"Dataset not loaded! Please run the dataset loading cell first.\")\n",
    "\n",
    "print(f\"‚úÖ Dataset verified: {len(dataset['train'])} train, {len(dataset['test'])} test examples\")\n",
    "\n",
    "# Create trainer with multiple fallback options\n",
    "trainer = None\n",
    "creation_method = \"\"\n",
    "\n",
    "# Option 1: Try with formatting function\n",
    "try:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=lambda x: x[\"text\"],\n",
    "    )\n",
    "    creation_method = \"SFTTrainer with formatting_func\"\n",
    "    print(\"‚úÖ Trainer created successfully with formatting_func\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SFTTrainer with formatting_func failed: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Option 2: Try without formatting function\n",
    "    try:\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            peft_config=lora_config,\n",
    "        )\n",
    "        creation_method = \"SFTTrainer without formatting_func\"\n",
    "        print(\"‚úÖ Trainer created successfully without formatting_func\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå SFTTrainer without formatting_func failed: {str(e2)[:100]}...\")\n",
    "        \n",
    "        # Option 3: Manual tokenization with standard Trainer\n",
    "        print(\"üîÑ Falling back to standard Trainer with manual tokenization...\")\n",
    "        \n",
    "        from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=1536,\n",
    "                return_tensors=None\n",
    "            )\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        tokenized_train = dataset[\"train\"].map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset[\"train\"].column_names,\n",
    "        )\n",
    "        \n",
    "        tokenized_eval = dataset[\"test\"].map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset[\"test\"].column_names,\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Create standard trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_eval,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        creation_method = \"Standard Trainer with manual tokenization\"\n",
    "        print(\"‚úÖ Standard Trainer created successfully\")\n",
    "\n",
    "if trainer is None:\n",
    "    raise RuntimeError(\"Failed to create trainer with any method!\")\n",
    "\n",
    "print(f\"\\nüéØ Trainer created using: {creation_method}\")\n",
    "print(\"üöÄ Ready for training!\")\n",
    "\n",
    "# Verify trainer is working\n",
    "try:\n",
    "    print(\"üîç Testing trainer setup...\")\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    print(f\"‚úÖ Sample batch shape: {sample_batch['input_ids'].shape}\")\n",
    "    print(\"‚úÖ Trainer setup verified successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Trainer test failed: {e}\")\n",
    "    print(\"Will attempt training anyway...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training process...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üîã GPU memory before training: {torch.cuda.memory_allocated()/1e9:.2f}GB allocated\")\n",
    "\n",
    "training_completed = False\n",
    "training_error = None\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    print(\"üèÉ‚Äç‚ôÇÔ∏è Beginning model training...\")\n",
    "    trainer.train()\n",
    "    training_completed = True\n",
    "    print(\"üéâ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    training_error = str(e)\n",
    "    print(f\"‚ùå Training failed with error: {training_error}\")\n",
    "    \n",
    "    # Try to save what we have\n",
    "    print(\"üíæ Attempting to save current model state...\")\n",
    "    try:\n",
    "        output_dir = \"./llama-3.1-8b-partial-save\"\n",
    "        trainer.model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"‚úÖ Partial model saved to {output_dir}\")\n",
    "    except Exception as save_error:\n",
    "        print(f\"‚ùå Could not save partial model: {save_error}\")\n",
    "\n",
    "# Final status\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if training_completed:\n",
    "    print(\"‚úÖ TRAINING STATUS: COMPLETED SUCCESSFULLY\")\n",
    "    \n",
    "    # Show training metrics if available\n",
    "    if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "        print(\"\\nüìä Final Training Metrics:\")\n",
    "        last_log = trainer.state.log_history[-1] if trainer.state.log_history else {}\n",
    "        for key, value in last_log.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"   {key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå TRAINING STATUS: FAILED\")\n",
    "    print(f\"Error: {training_error}\")\n",
    "    print(\"Check the error above and try adjusting batch size or other parameters\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model...\")\n",
    "try:\n",
    "    output_dir = \"./llama-3.1-8b-corporate-assistant-final\"\n",
    "    \n",
    "    # Save the model state (PEFT adapters)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "    \n",
    "    # Save training metrics if available\n",
    "    if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "        with open(f\"{output_dir}/training_metrics.json\", \"w\") as f:\n",
    "            json.dump(trainer.state.log_history, f, indent=2)\n",
    "        print(\"‚úÖ Training metrics saved!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING PROCESS COMPLETED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
