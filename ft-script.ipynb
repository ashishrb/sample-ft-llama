{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3B: Fix PyTorch/NCCL Compatibility Issue\n",
    "# ============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=== PYTORCH/NCCL COMPATIBILITY FIX ===\")\n",
    "print(\"Issue: ncclMemFree symbol not found\")\n",
    "print(\"Solution: Reinstall PyTorch with proper CUDA compatibility\")\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    \"\"\"Run command with proper error handling\"\"\"\n",
    "    print(f\"\\nüîÑ {description}...\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=600)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {description} completed successfully\")\n",
    "            if result.stdout.strip():\n",
    "                print(f\"Output: {result.stdout.strip()[:200]}...\")\n",
    "        else:\n",
    "            print(f\"‚ùå {description} failed\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "        return result.returncode == 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"‚è∞ {description} timed out\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in {description}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Step 1: Clean uninstall PyTorch and related packages\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 1: Clean uninstall conflicting packages\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "uninstall_packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"torchaudio\",\n",
    "    \"transformers\",\n",
    "    \"accelerate\"\n",
    "]\n",
    "\n",
    "for package in uninstall_packages:\n",
    "    cmd = f\"{sys.executable} -m pip uninstall {package} -y\"\n",
    "    run_command(cmd, f\"Uninstalling {package}\")\n",
    "\n",
    "# Step 2: Clear pip cache\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2: Clear pip cache\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "run_command(f\"{sys.executable} -m pip cache purge\", \"Clearing pip cache\")\n",
    "\n",
    "# Step 3: Install PyTorch with specific CUDA version for H100\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 3: Install PyTorch with CUDA 12.4 compatibility\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Install PyTorch with specific CUDA version\n",
    "pytorch_cmd = f\"{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\"\n",
    "run_command(pytorch_cmd, \"Installing PyTorch with CUDA 12.4\")\n",
    "\n",
    "# Step 4: Install transformers and accelerate\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 4: Install core packages\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "core_packages = [\n",
    "    \"transformers>=4.43.0\",\n",
    "    \"accelerate>=0.21.0\"\n",
    "]\n",
    "\n",
    "for package in core_packages:\n",
    "    cmd = f\"{sys.executable} -m pip install {package}\"\n",
    "    run_command(cmd, f\"Installing {package}\")\n",
    "\n",
    "# Step 5: Install remaining packages\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 5: Install remaining packages\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "remaining_packages = [\n",
    "    \"bitsandbytes>=0.41.0\",\n",
    "    \"scipy\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\"\n",
    "]\n",
    "\n",
    "for package in remaining_packages:\n",
    "    cmd = f\"{sys.executable} -m pip install {package}\"\n",
    "    run_command(cmd, f\"Installing {package}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PYTORCH REINSTALLATION COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"üîÑ Please RESTART your kernel now\")\n",
    "print(\"üîÑ Then run the verification script again\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22672145",
   "metadata": {},
   "source": [
    "## Verify Installation and Test Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== STEP 3: VERIFICATION ===\")\n",
    "\n",
    "# Test 1: Basic imports\n",
    "print(\"\\nüîç Test 1: Basic package imports...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} - CUDA: {torch.cuda.is_available()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå PyTorch import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"‚úÖ Transformers {transformers.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Transformers import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"‚úÖ Accelerate {accelerate.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Accelerate import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(f\"‚úÖ PEFT {peft.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå PEFT import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(f\"‚úÖ BitsAndBytes {bitsandbytes.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå BitsAndBytes import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import trl\n",
    "    print(f\"‚úÖ TRL {trl.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå TRL import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"‚úÖ Datasets {datasets.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Datasets import failed: {e}\")\n",
    "\n",
    "# Test 2: Advanced imports\n",
    "print(\"\\nüîç Test 2: Advanced component imports...\")\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    print(\"‚úÖ Core transformers components\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Transformers components failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "    print(\"‚úÖ PEFT components\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå PEFT components failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from trl import SFTTrainer\n",
    "    print(\"‚úÖ TRL SFTTrainer\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå TRL SFTTrainer failed: {e}\")\n",
    "\n",
    "# Test 3: GPU and CUDA compatibility\n",
    "print(\"\\nüîç Test 3: GPU compatibility...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Test tensor operations\n",
    "    try:\n",
    "        x = torch.randn(100, 100).cuda()\n",
    "        y = torch.randn(100, 100).cuda()\n",
    "        z = torch.matmul(x, y)\n",
    "        print(\"‚úÖ Basic CUDA tensor operations work\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CUDA tensor operations failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available\")\n",
    "\n",
    "# Test 4: BitsAndBytes compatibility\n",
    "print(\"\\nüîç Test 4: BitsAndBytes compatibility...\")\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    # Test if BitsAndBytes can create quantization config\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    print(\"‚úÖ BitsAndBytes quantization config works\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå BitsAndBytes compatibility issue: {e}\")\n",
    "\n",
    "# Test 5: Check Llama-3.1 model accessibility\n",
    "print(\"\\nüîç Test 5: Llama-3.1 model access...\")\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    # Try to load tokenizer (this tests HF access and model compatibility)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        token=\"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\"\n",
    "    )\n",
    "    print(\"‚úÖ Llama-3.1 tokenizer loads successfully\")\n",
    "    print(f\"‚úÖ Vocab size: {tokenizer.vocab_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Llama-3.1 access issue: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if all critical components are working\n",
    "critical_tests = [\n",
    "    torch.cuda.is_available(),\n",
    "    'transformers' in globals(),\n",
    "    'bitsandbytes' in globals(),\n",
    "    'trl' in globals(),\n",
    "    'peft' in globals()\n",
    "]\n",
    "\n",
    "if all(critical_tests):\n",
    "    print(\"üéâ ALL CRITICAL TESTS PASSED!\")\n",
    "    print(\"‚úÖ Ready to proceed with Step 4: Dataset preparation\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some tests failed. Please review errors above.\")\n",
    "    print(\"‚ùå NOT ready for training yet\")\n",
    "\n",
    "print(\"\\nNext: If all tests pass, we'll proceed to Step 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04491b33",
   "metadata": {},
   "source": [
    "## Validate with sample training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Dataset Preparation and Processing\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "print(\"=== STEP 4: DATASET PREPARATION ===\")\n",
    "\n",
    "# First, let's create your dataset file from the provided data\n",
    "def create_dataset_file():\n",
    "    \"\"\"Create the JSONL dataset file from your provided data\"\"\"\n",
    "    \n",
    "    # Your dataset entries (sample from the document you provided)\n",
    "    dataset_entries = [\n",
    "        {\n",
    "            \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "            \"instruction\": \"Under Job Code for the same grade FTE we can see - PM and FC job codes. what is the difference between two\",\n",
    "            \"output\": \"PM job code indicates Full Time Employee and FC job code indicates Full Time Contractor.\"\n",
    "        },\n",
    "        {\n",
    "            \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "            \"instruction\": \"Process to convert CWR to FTE associates\",\n",
    "            \"output\": \"Raise New Demand New Demand and mark requirement type as CWR Conversion and SO type as FTE. Rest of the Information will remain same like FTE SO creation.\"\n",
    "        },\n",
    "        {\n",
    "            \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "            \"instruction\": \"How to created CWR so\",\n",
    "            \"output\": \"To create a CWR Contractor Workforce Request SO follow these steps: 1. In OneC open the Quick SOgtgt Raise new demand gtgt New demand . 2. Fill in the fields of Project amp Assignment and Location as we fill in for a regular Staffing SO 3. Select SO type as CWRgtgt Service defauls to Custom service gtgt Select grade and choose a Non CCA demand role that is closer to your requirement For the list of NON CCA role codes and job codes Please refer to the worksheet embedded in the PM Handbook gtgt Select the job code with CW00 available in drop down list based on grade selection amp add the Technical skill individually. 4. Fill in the reset of the details and click on Submit to raise the SO request.  If you need any further assistance with creating a CWR SO please reach out to the SOSimplificationPM@cognizant.com  Explore additional resources about SO Simplification on Be.Cognizant.\"\n",
    "        },\n",
    "        {\n",
    "            \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "            \"instruction\": \"unable to select subcontractor\",\n",
    "            \"output\": \"To create a CWR Contractor Workforce Request SO follow these steps: 1. In OneC open the Quick SOgtgt Raise new demand gtgt New demand . 2. Fill in the fields of Project amp Assignment and Location as we fill in for a regular Staffing SO 3. Select SO type as CWRgtgt service defaults to Custom service gtgt Select grade and choose a Non CCA demand role that is closer to your requirement For the list of NON CCA role codes and job codes Please refer to the worksheet embedded in the PM Handbook gtgt Select the job code with CW00 available in drop down list based on grade selection amp add the Technical skill individually. 4. Fill in the reset of the details and click on Submit to raise the SO request.  If you need any further assistance with creating a CWR SO please reach out to the SOSimplificationPM@cognizant.com Explore additional resources about SO Simplification on Be.Cognizant.\"\n",
    "        },\n",
    "        {\n",
    "            \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "            \"instruction\": \"How to raise request for CWR SO\",\n",
    "            \"output\": \"To create a CWR Contractor Workforce Request SO follow these steps: 1. In OneC open the Quick SOgtgt Raise new demand gtgt New demand . 2. Fill in the fields of Project amp Assignment and Location as we fill in for a regular Staffing SO 3. Select SO type as CWRgtgt Service defauls to Custom service gtgt Select grade and choose a Non CCA demand role that is closer to your requirement For the list of NON CCA role codes and job codes Please refer to the worksheet embedded in the PM Handbook gtgt Select the job code with CW00 available in drop down list based on grade selection amp add the Technical skill individually. 4. Fill in the reset of the details and click on Submit to raise the SO request.  If you need any further assistance with creating a CWR SO please reach out to the SOSimplificationPM@cognizant.com .Explore additional resources about SO Simplification on Be.Cognizant.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save to JSONL file\n",
    "    with open('azure_instruction_dataset.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for entry in dataset_entries:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"‚úÖ Created dataset file with {len(dataset_entries)} sample entries\")\n",
    "    print(\"üìÅ File saved as: azure_instruction_dataset.jsonl\")\n",
    "    \n",
    "    return len(dataset_entries)\n",
    "\n",
    "# Create the dataset file\n",
    "num_entries = create_dataset_file()\n",
    "\n",
    "# Data processing functions\n",
    "def convert_to_conversational_format(data_entry):\n",
    "    \"\"\"Convert system/instruction/output format to Llama-3.1 conversational format\"\"\"\n",
    "    system_message = data_entry.get('system', '')\n",
    "    instruction = data_entry.get('instruction', '')\n",
    "    output = data_entry.get('output', '')\n",
    "    \n",
    "    # Create conversational format suitable for Llama-3.1\n",
    "    conversation = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "def load_and_process_dataset(file_path, test_size=0.2):\n",
    "    \"\"\"Load and process the dataset\"\"\"\n",
    "    print(f\"\\nüîÑ Loading dataset from {file_path}...\")\n",
    "    \n",
    "    # Load JSONL data\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    entry = json.loads(line.strip())\n",
    "                    data.append(entry)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è  Skipping malformed line {line_num}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(data)} entries\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File {file_path} not found!\")\n",
    "        return None\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"‚ùå No valid data found!\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to conversational format\n",
    "    print(\"üîÑ Converting to conversational format...\")\n",
    "    conversations = []\n",
    "    \n",
    "    for i, entry in enumerate(data):\n",
    "        try:\n",
    "            conv = convert_to_conversational_format(entry)\n",
    "            conversations.append({\"text\": conv})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Skipping entry {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Converted {len(conversations)} conversations\")\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    dataset = Dataset.from_list(conversations)\n",
    "    \n",
    "    # Split into train/validation\n",
    "    if len(conversations) > 1:\n",
    "        dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "        print(f\"üìä Train examples: {len(dataset['train'])}\")\n",
    "        print(f\"üìä Validation examples: {len(dataset['test'])}\")\n",
    "    else:\n",
    "        # For single example, create minimal split\n",
    "        dataset = DatasetDict({\n",
    "            'train': dataset,\n",
    "            'test': dataset.select([0])  # Use same example for validation\n",
    "        })\n",
    "        print(f\"üìä Single example mode - using same data for train/validation\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset = load_and_process_dataset('azure_instruction_dataset.jsonl')\n",
    "\n",
    "if dataset is not None:\n",
    "    print(\"\\n‚úÖ Dataset processing completed successfully!\")\n",
    "    \n",
    "    # Show sample conversation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLE CONVERSATION:\")\n",
    "    print(\"=\"*50)\n",
    "    sample_text = dataset['train'][0]['text']\n",
    "    print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)\n",
    "    \n",
    "    # Show dataset statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATASET STATISTICS:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üìà Total training examples: {len(dataset['train'])}\")\n",
    "    print(f\"üìà Total validation examples: {len(dataset['test'])}\")\n",
    "    \n",
    "    # Calculate average text length\n",
    "    lengths = [len(example['text']) for example in dataset['train']]\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    min_length = min(lengths)\n",
    "    \n",
    "    print(f\"üìè Average text length: {avg_length:.0f} characters\")\n",
    "    print(f\"üìè Max text length: {max_length} characters\")\n",
    "    print(f\"üìè Min text length: {min_length} characters\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Ready for Step 5: Model loading and training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset processing failed!\")\n",
    "    print(\"Please check the dataset file and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5e1ab",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install -q transformers>=4.43.0\n",
    "%pip install -q accelerate>=0.21.0\n",
    "%pip install -q peft>=0.4.0\n",
    "%pip install -q bitsandbytes>=0.41.0\n",
    "%pip install -q trl>=0.7.0\n",
    "%pip install -q datasets\n",
    "%pip install -q scipy\n",
    "%pip install -q tensorboard\n",
    "%pip install -q wandb\n",
    "%pip install -q sentencepiece\n",
    "%pip install -q protobuf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c7568",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa6335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA H100 NVL\n",
      "GPU Memory: 99.87 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup device and check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\" if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82159dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# QLoRA configuration - optimized for H100\n",
    "qlora_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA configuration - balanced for instruction tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                    # Rank - higher for better performance\n",
    "    lora_alpha=16,           # Alpha parameter for LoRA scaling\n",
    "    target_modules=[         # Target all attention layers\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,        # Dropout for regularization\n",
    "    bias=\"none\",             # No bias training\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False\n",
    ")\n",
    "\n",
    "# Training hyperparameters - optimized for H100 with 40 cores\n",
    "training_config = {\n",
    "    \"output_dir\": \"./llama-3.1-8b-corporate-assistant\",\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 2,    # Optimal for H100\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,     # Effective batch size = 4*4*4 = 64\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"learning_rate\": 3e-4,               # Optimal for QLoRA\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,                        # Use bf16 for H100\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"max_steps\": -1,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"group_by_length\": True,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"report_to\": \"tensorboard\",\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 100,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"dataloader_num_workers\": 8,         # Utilize multiple cores\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"push_to_hub\": False,\n",
    "    \"hub_model_id\": None,\n",
    "    \"hub_strategy\": \"every_save\"\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sample_dataset():\n",
    "#     \"\"\"Create the azure_instruction_dataset.jsonl file with sample corporate data\"\"\"\n",
    "    \n",
    "#     print(\"üìù Creating azure_instruction_dataset.jsonl with sample data...\")\n",
    "    \n",
    "#     # Sample dataset entries from your corporate assistant domain\n",
    "#     dataset_entries = [\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"Under Job Code for the same grade FTE we can see - PM and FC job codes. what is the difference between two\",\n",
    "#             \"output\": \"PM job code indicates Full Time Employee and FC job code indicates Full Time Contractor.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"Process to convert CWR to FTE associates\",\n",
    "#             \"output\": \"Raise New Demand New Demand and mark requirement type as CWR Conversion and SO type as FTE. Rest of the Information will remain same like FTE SO creation.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"How to created CWR so\",\n",
    "#             \"output\": \"To create a CWR Contractor Workforce Request SO follow these steps: 1. In OneC open the Quick SOgtgt Raise new demand gtgt New demand . 2. Fill in the fields of Project amp Assignment and Location as we fill in for a regular Staffing SO 3. Select SO type as CWRgtgt Service defauls to Custom service gtgt Select grade and choose a Non CCA demand role that is closer to your requirement For the list of NON CCA role codes and job codes Please refer to the worksheet embedded in the PM Handbook gtgt Select the job code with CW00 available in drop down list based on grade selection amp add the Technical skill individually. 4. Fill in the reset of the details and click on Submit to raise the SO request.  If you need any further assistance with creating a CWR SO please reach out to the SOSimplificationPM@cognizant.com  Explore additional resources about SO Simplification on Be.Cognizant.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"unable to select subcontractor\",\n",
    "#             \"output\": \"To create a CWR Contractor Workforce Request SO follow these steps: 1. In OneC open the Quick SOgtgt Raise new demand gtgt New demand . 2. Fill in the fields of Project amp Assignment and Location as we fill in for a regular Staffing SO 3. Select SO type as CWRgtgt service defaults to Custom service gtgt Select grade and choose a Non CCA demand role that is closer to your requirement For the list of NON CCA role codes and job codes Please refer to the worksheet embedded in the PM Handbook gtgt Select the job code with CW00 available in drop down list based on grade selection amp add the Technical skill individually. 4. Fill in the reset of the details and click on Submit to raise the SO request.  If you need any further assistance with creating a CWR SO please reach out to the SOSimplificationPM@cognizant.com Explore additional resources about SO Simplification on Be.Cognizant.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"How to raise request for CWR SO\",\n",
    "#             \"output\": \"To create a CWR Contractor Workforce Request SO follow these steps: 1. In OneC open the Quick SOgtgt Raise new demand gtgt New demand . 2. Fill in the fields of Project amp Assignment and Location as we fill in for a regular Staffing SO 3. Select SO type as CWRgtgt Service defauls to Custom service gtgt Select grade and choose a Non CCA demand role that is closer to your requirement For the list of NON CCA role codes and job codes Please refer to the worksheet embedded in the PM Handbook gtgt Select the job code with CW00 available in drop down list based on grade selection amp add the Technical skill individually. 4. Fill in the reset of the details and click on Submit to raise the SO request.  If you need any further assistance with creating a CWR SO please reach out to the SOSimplificationPM@cognizant.com .Explore additional resources about SO Simplification on Be.Cognizant.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"Unable to create opportunity id in winzone\",\n",
    "#             \"output\": \"To create new winzone opportunity please reach out Account Manager/ Client Partner/ CRM for further assistance.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"How to view hardcopy of I-140 approval notice\",\n",
    "#             \"output\": \"1.  I-140 approval notices are shipped to cognizant office within two to four weeks once the status updated in the USCIS site. 2.  As the I-140 is a company-owned petition original hardcopy cannot be shared.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"Are there any migration benefits available in Google\",\n",
    "#             \"output\": \"Yes we have multiple benefits and credits available for migration from on-perm to google cloud. Please reach out to ProcurementITCloud@cognizant.com for detailed information.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"What equipment do I need for a telemedicine appointment\",\n",
    "#             \"output\": \"Youll need a device with a camera and microphone such as a smartphone tablet or computer and a reliable internet connection. Available at MHC.\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"system\": \"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\",\n",
    "#             \"instruction\": \"Do we still need to validate the SOs through email for GGM SOs\",\n",
    "#             \"output\": \"Currently for APAC ASEAN amp UK region Pre-defined templates details for GGM SOs has been removed from validation procedure. Region wise process will be automated soon for GGM SOs validation.\"\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # Write to JSONL file\n",
    "#     filename = \"azure_instruction_dataset.jsonl\"\n",
    "#     with open(filename, 'w', encoding='utf-8') as f:\n",
    "#         for entry in dataset_entries:\n",
    "#             f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "#     print(f\"‚úÖ Created {filename} with {len(dataset_entries)} training examples\")\n",
    "#     return len(dataset_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_dataset(file_path, test_size=0.1):\n",
    "#     \"\"\"Load and preprocess the dataset\"\"\"\n",
    "#     print(f\"üîÑ Loading dataset from {file_path}...\")\n",
    "    \n",
    "#     # Check if file exists\n",
    "#     if not os.path.exists(file_path):\n",
    "#         print(f\"‚ùå File {file_path} not found!\")\n",
    "#         return None\n",
    "    \n",
    "#     # Load JSONL data\n",
    "#     raw_data = []\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             for line_num, line in enumerate(f, 1):\n",
    "#                 try:\n",
    "#                     entry = json.loads(line.strip())\n",
    "#                     raw_data.append(entry)\n",
    "#                 except json.JSONDecodeError as e:\n",
    "#                     print(f\"‚ö†Ô∏è  Skipping malformed line {line_num}: {e}\")\n",
    "#                     continue\n",
    "        \n",
    "#         print(f\"‚úÖ Loaded {len(raw_data)} examples\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error loading file: {e}\")\n",
    "#         return None\n",
    "    \n",
    "#     if len(raw_data) == 0:\n",
    "#         print(\"‚ùå No valid data found!\")\n",
    "#         return None\n",
    "    \n",
    "#     # Convert to conversational format\n",
    "#     print(\"üîÑ Converting to conversational format...\")\n",
    "#     conversations = []\n",
    "#     for entry in raw_data:\n",
    "#         try:\n",
    "#             conv = convert_to_conversational_format(entry)\n",
    "#             conversations.append({\"text\": conv})\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è  Error converting entry: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     print(f\"‚úÖ Converted {len(conversations)} conversations\")\n",
    "    \n",
    "#     # Create dataset\n",
    "#     dataset = Dataset.from_list(conversations)\n",
    "    \n",
    "#     # Split into train/validation\n",
    "#     if len(conversations) > 1:\n",
    "#         dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "#     else:\n",
    "#         # For single example, duplicate for validation\n",
    "#         dataset = DatasetDict({\n",
    "#             'train': dataset,\n",
    "#             'test': dataset.select([0])\n",
    "#         })\n",
    "    \n",
    "#     print(f\"üìä Train examples: {len(dataset['train'])}\")\n",
    "#     print(f\"üìä Validation examples: {len(dataset['test'])}\")\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# # Step 5D: Execute dataset creation and loading\n",
    "# print(\"Starting dataset creation and processing...\")\n",
    "\n",
    "# # Create the dataset file\n",
    "# num_entries = create_sample_dataset()\n",
    "\n",
    "# # Set dataset path\n",
    "# DATASET_PATH = \"./azure_instruction_dataset.jsonl\"\n",
    "\n",
    "# # Load and preprocess dataset\n",
    "# dataset = preprocess_dataset(DATASET_PATH)\n",
    "\n",
    "# if dataset is not None:\n",
    "#     print(\"\\n‚úÖ Dataset processing completed successfully!\")\n",
    "    \n",
    "#     # Display sample\n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"SAMPLE CONVERSATION:\")\n",
    "#     print(\"=\"*50)\n",
    "#     sample_text = dataset['train'][0]['text']\n",
    "#     print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)\n",
    "    \n",
    "#     # Calculate statistics\n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"DATASET STATISTICS:\")\n",
    "#     print(\"=\"*50)\n",
    "    \n",
    "#     # Text length analysis\n",
    "#     train_lengths = [len(example['text']) for example in dataset['train']]\n",
    "#     avg_length = sum(train_lengths) / len(train_lengths)\n",
    "#     max_length = max(train_lengths)\n",
    "#     min_length = min(train_lengths)\n",
    "    \n",
    "#     print(f\"üìà Total training examples: {len(dataset['train'])}\")\n",
    "#     print(f\"üìà Total validation examples: {len(dataset['test'])}\")\n",
    "#     print(f\"üìè Average text length: {avg_length:.0f} characters\")\n",
    "#     print(f\"üìè Max text length: {max_length} characters\")\n",
    "#     print(f\"üìè Min text length: {min_length} characters\")\n",
    "    \n",
    "#     # Estimate token count (rough approximation: 1 token ‚âà 4 characters)\n",
    "#     avg_tokens = avg_length // 4\n",
    "#     max_tokens = max_length // 4\n",
    "    \n",
    "#     print(f\"üî§ Estimated avg tokens: {avg_tokens}\")\n",
    "#     print(f\"üî§ Estimated max tokens: {max_tokens}\")\n",
    "    \n",
    "#     if max_tokens > 2048:\n",
    "#         print(\"‚ö†Ô∏è  Warning: Some examples may exceed 2048 tokens\")\n",
    "#         print(\"üí° Consider reducing max_seq_length in training config\")\n",
    "    \n",
    "#     print(\"\\nüéØ Recommended max_seq_length for training: \" + str(min(2048, max_tokens + 100)))\n",
    "    \n",
    "# else:\n",
    "#     print(\"‚ùå Dataset creation failed!\")\n",
    "#     print(\"Please check the errors above and try again.\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"DATASET PREPARATION COMPLETE!\")\n",
    "# print(\"=\"*50)\n",
    "# if dataset is not None:\n",
    "#     print(\"‚úÖ Ready to proceed with model loading and training\")\n",
    "#     print(\"‚úÖ Dataset variable 'dataset' is now available for training\")\n",
    "# else:\n",
    "#     print(\"‚ùå Dataset preparation failed - cannot proceed with training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3283355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_data(file_path):\n",
    "    \"\"\"Load data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a587b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conversational_format(data_entry):\n",
    "    \"\"\"Convert system/instruction/output format to conversational format\"\"\"\n",
    "    system_message = data_entry.get('system', '')\n",
    "    instruction = data_entry.get('instruction', '')\n",
    "    output = data_entry.get('output', '')\n",
    "    \n",
    "    # Create conversational format suitable for Llama-3.1\n",
    "    conversation = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1311705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(file_path, test_size=0.1):\n",
    "    \"\"\"Load and preprocess the dataset\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    raw_data = load_jsonl_data(file_path)\n",
    "    print(f\"Loaded {len(raw_data)} examples\")\n",
    "    \n",
    "    # Convert to conversational format\n",
    "    print(\"Converting to conversational format...\")\n",
    "    conversations = []\n",
    "    for entry in raw_data:\n",
    "        conv = convert_to_conversational_format(entry)\n",
    "        conversations.append({\"text\": conv})\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(conversations)\n",
    "    \n",
    "    # Split into train/validation\n",
    "    dataset = dataset.train_test_split(test_size=test_size, seed=42)\n",
    "    \n",
    "    print(f\"Train examples: {len(dataset['train'])}\")\n",
    "    print(f\"Validation examples: {len(dataset['test'])}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbae02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 1186 examples\n",
      "Converting to conversational format...\n",
      "Train examples: 1067\n",
      "Validation examples: 119\n",
      "\n",
      "=== Sample Conversation ===\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How do customers gain access to a subscription in CSP<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Workloads/ subscriptions are hosted  on customer domain. Customer has access to the workloads through Azure Portal.<|eot_id|>...\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset (replace with your file path)\n",
    "DATASET_PATH = \"/home/azureuser/cloudfiles/code/Users/746582/llama-8b-ft-11th-june/azure_instruction_dataset.jsonl\"  # Update this path\n",
    "dataset = preprocess_dataset(DATASET_PATH)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n=== Sample Conversation ===\")\n",
    "print(dataset['train'][0]['text'][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2fe84db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model with QLoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [03:05<00:00, 46.30s/it] \n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model with QLoRA...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=qlora_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637cdb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 167,772,160 || All params: 4,708,372,480 || Trainable%: 3.56\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}\")\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cd853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRL VERSION DIAGNOSIS ===\n",
      "TRL version: 0.18.1\n",
      "\n",
      "=== SFTTrainer ACCEPTED PARAMETERS ===\n",
      "Accepted parameters:\n",
      "  ‚Ä¢ self\n",
      "  ‚Ä¢ model\n",
      "  ‚Ä¢ args\n",
      "  ‚Ä¢ data_collator\n",
      "  ‚Ä¢ train_dataset\n",
      "  ‚Ä¢ eval_dataset\n",
      "  ‚Ä¢ processing_class\n",
      "  ‚Ä¢ compute_loss_func\n",
      "  ‚Ä¢ compute_metrics\n",
      "  ‚Ä¢ callbacks\n",
      "  ‚Ä¢ optimizers\n",
      "  ‚Ä¢ optimizer_cls_and_kwargs\n",
      "  ‚Ä¢ preprocess_logits_for_metrics\n",
      "  ‚Ä¢ peft_config\n",
      "  ‚Ä¢ formatting_func\n",
      "\n",
      "=== CORRECT TRAINER INITIALIZATION ===\n",
      "Using TRL 0.18.1 (newer) syntax:\n",
      "\n",
      "# Try this first:\n",
      "trainer = SFTTrainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=dataset[\"train\"],\n",
      "    eval_dataset=dataset[\"test\"],\n",
      "    tokenizer=tokenizer,\n",
      "    peft_config=lora_config,\n",
      "    max_seq_length=2048,\n",
      "    packing=False,\n",
      ")\n",
      "\n",
      "# If above fails, try this minimal version:\n",
      "trainer = SFTTrainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=dataset[\"train\"],\n",
      "    tokenizer=tokenizer,\n",
      ")\n",
      "\n",
      "\n",
      "=== ALTERNATIVE: Use DataCollatorForLanguageModeling ===\n",
      "\n",
      "# If SFTTrainer keeps failing, use standard Trainer:\n",
      "from transformers import Trainer, DataCollatorForLanguageModeling\n",
      "\n",
      "# Tokenize datasets\n",
      "def tokenize_function(examples):\n",
      "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=2048)\n",
      "\n",
      "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
      "tokenized_eval = dataset[\"test\"].map(tokenize_function, batched=True)\n",
      "\n",
      "# Data collator\n",
      "data_collator = DataCollatorForLanguageModeling(\n",
      "    tokenizer=tokenizer,\n",
      "    mlm=False,\n",
      ")\n",
      "\n",
      "# Use standard Trainer\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=tokenized_train,\n",
      "    eval_dataset=tokenized_eval,\n",
      "    data_collator=data_collator,\n",
      ")\n",
      "\n",
      "\n",
      "==================================================\n",
      "RECOMMENDED ACTION:\n",
      "==================================================\n",
      "1. Run this diagnostic script first\n",
      "2. Check your TRL version\n",
      "3. Use the appropriate syntax shown above\n",
      "4. If SFTTrainer still fails, use the standard Trainer alternative\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRL Version Diagnosis and Correct SFTTrainer Syntax\n",
    "# ============================================================================\n",
    "\n",
    "import trl\n",
    "from trl import SFTTrainer\n",
    "import inspect\n",
    "\n",
    "print(\"=== TRL VERSION DIAGNOSIS ===\")\n",
    "print(f\"TRL version: {trl.__version__}\")\n",
    "\n",
    "# Check SFTTrainer parameters\n",
    "print(\"\\n=== SFTTrainer ACCEPTED PARAMETERS ===\")\n",
    "sig = inspect.signature(SFTTrainer.__init__)\n",
    "params = list(sig.parameters.keys())\n",
    "print(\"Accepted parameters:\")\n",
    "for param in params:\n",
    "    print(f\"  ‚Ä¢ {param}\")\n",
    "\n",
    "print(\"\\n=== CORRECT TRAINER INITIALIZATION ===\")\n",
    "\n",
    "# Version-specific fixes\n",
    "trl_version = trl.__version__\n",
    "\n",
    "if trl_version.startswith('0.4') or trl_version.startswith('0.5') or trl_version.startswith('0.6'):\n",
    "    print(\"Using TRL 0.4-0.6 syntax:\")\n",
    "    print(\"\"\"\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "elif trl_version.startswith('0.7') or trl_version.startswith('0.8'):\n",
    "    print(\"Using TRL 0.7-0.8 syntax:\")\n",
    "    print(\"\"\"\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=lambda x: x[\"text\"],\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "else:\n",
    "    print(f\"Using TRL {trl_version} (newer) syntax:\")\n",
    "    print(\"\"\"\n",
    "# Try this first:\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# If above fails, try this minimal version:\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== ALTERNATIVE: Use DataCollatorForLanguageModeling ===\")\n",
    "print(\"\"\"\n",
    "# If SFTTrainer keeps failing, use standard Trainer:\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=2048)\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_eval = dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Use standard Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RECOMMENDED ACTION:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Run this diagnostic script first\")\n",
    "print(\"2. Check your TRL version\")\n",
    "print(\"3. Use the appropriate syntax shown above\")\n",
    "print(\"4. If SFTTrainer still fails, use the standard Trainer alternative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "968de5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1067/1067 [00:00<00:00, 35104.97 examples/s]\n",
      "Converting train dataset to ChatML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1067/1067 [00:00<00:00, 74317.45 examples/s]\n",
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1067/1067 [00:00<00:00, 62758.69 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1067/1067 [00:00<00:00, 4423.63 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1067/1067 [00:00<00:00, 476148.78 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119/119 [00:00<00:00, 26963.54 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119/119 [00:00<00:00, 44018.18 examples/s]\n",
      "Adding EOS to eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119/119 [00:00<00:00, 42813.71 examples/s]\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119/119 [00:00<00:00, 4162.82 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119/119 [00:00<00:00, 83090.09 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create training arguments\n",
    "training_args = TrainingArguments(**training_config)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=lambda x: x[\"text\"],  # This tells it which field to use\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Fix: TypeError: 'method' object is not subscriptable\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== DEBUGGING TRAINER ERROR ===\")\n",
    "\n",
    "# Step 1: Check dataset format\n",
    "print(\"üîç Checking dataset format...\")\n",
    "print(\"Sample from train dataset:\")\n",
    "print(dataset[\"train\"][0])\n",
    "print(\"\\nDataset columns:\", dataset[\"train\"].column_names)\n",
    "\n",
    "# Step 2: Fix the formatting function\n",
    "print(\"\\nüîß Creating proper formatting function...\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Proper formatting function for TRL 0.18.1\"\"\"\n",
    "    if isinstance(examples, dict):\n",
    "        # Single example\n",
    "        if \"text\" in examples:\n",
    "            return examples[\"text\"]\n",
    "        else:\n",
    "            return str(examples)\n",
    "    else:\n",
    "        # Batch of examples\n",
    "        texts = []\n",
    "        for example in examples:\n",
    "            if isinstance(example, dict) and \"text\" in example:\n",
    "                texts.append(example[\"text\"])\n",
    "            else:\n",
    "                texts.append(str(example))\n",
    "        return texts\n",
    "\n",
    "# Step 3: Alternative - Process dataset first\n",
    "print(\"üîÑ Processing dataset for compatibility...\")\n",
    "\n",
    "def process_dataset_for_sft(dataset):\n",
    "    \"\"\"Process dataset to ensure compatibility\"\"\"\n",
    "    \n",
    "    # Check if it's already in the right format\n",
    "    if \"text\" in dataset.column_names:\n",
    "        print(\"‚úÖ Dataset already has 'text' column\")\n",
    "        return dataset\n",
    "    \n",
    "    # If not, create it\n",
    "    def add_text_column(examples):\n",
    "        # Assuming the dataset has the conversation format\n",
    "        return {\"text\": examples.get(\"text\", str(examples))}\n",
    "    \n",
    "    return dataset.map(add_text_column)\n",
    "\n",
    "# Process datasets\n",
    "processed_train = process_dataset_for_sft(dataset[\"train\"])\n",
    "processed_eval = process_dataset_for_sft(dataset[\"test\"])\n",
    "\n",
    "print(\"‚úÖ Datasets processed\")\n",
    "\n",
    "# Step 4: Create trainer with better error handling\n",
    "print(\"\\nüèãÔ∏è Creating trainer with improved setup...\")\n",
    "\n",
    "try:\n",
    "    # Option 1: Simple formatting function\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_train,\n",
    "        eval_dataset=processed_eval,\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "    )\n",
    "    print(\"‚úÖ Trainer created successfully with formatting_func\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Option 1 failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Option 2: Without formatting function (auto-detect)\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_train,\n",
    "            eval_dataset=processed_eval,\n",
    "            peft_config=lora_config,\n",
    "        )\n",
    "        print(\"‚úÖ Trainer created successfully without formatting_func\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Option 2 failed: {e2}\")\n",
    "        \n",
    "        # Option 3: Manual tokenization approach\n",
    "        print(\"üîÑ Falling back to manual tokenization...\")\n",
    "        \n",
    "        from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "        \n",
    "        # Tokenize the datasets manually\n",
    "        def tokenize_function(examples):\n",
    "            # Handle both single and batch\n",
    "            if isinstance(examples[\"text\"], str):\n",
    "                texts = [examples[\"text\"]]\n",
    "            else:\n",
    "                texts = examples[\"text\"]\n",
    "            \n",
    "            return tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                padding=False,  # Will be done by data collator\n",
    "                max_length=1536,  # Based on your data analysis\n",
    "                return_tensors=None\n",
    "            )\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        tokenized_train = processed_train.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=processed_train.column_names,\n",
    "        )\n",
    "        \n",
    "        tokenized_eval = processed_eval.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=processed_eval.column_names,\n",
    "        )\n",
    "        \n",
    "        # Data collator for language modeling\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,  # We're doing causal LM, not masked LM\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Create standard trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_eval,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Trainer created with manual tokenization approach\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINER SETUP COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Now try: trainer.train()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80ab8978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='671' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 15:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.283500</td>\n",
       "      <td>1.643767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.137900</td>\n",
       "      <td>1.516374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>1.597736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>1.678621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>1.878496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:2686\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2684\u001b[0m         smp\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[0;32m-> 2686\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2688\u001b[0m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_loss_scalar \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:2965\u001b[0m, in \u001b[0;36mTrainer._load_best_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m   2961\u001b[0m     model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2962\u001b[0m ):\n\u001b[1;32m   2963\u001b[0m     \u001b[38;5;66;03m# For BC for older PEFT versions\u001b[39;00m\n\u001b[1;32m   2964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 2965\u001b[0m         active_adapter \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2966\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mactive_adapters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2967\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected multiple active adapters, will only consider the first one\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952037ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./llama-3.1-8b-corporate-assistant-final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# Save training metrics\n",
    "import json\n",
    "with open(f\"{output_dir}/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "print(\"Training metrics saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for testing\n",
    "print(\"Loading fine-tuned model for testing...\")\n",
    "\n",
    "# Create pipeline for inference\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(instruction, system_prompt=\"You are a helpful corporate AI assistant that helps employees with internal processes, applications, and workplace questions.\"):\n",
    "    \"\"\"Test the fine-tuned model with a sample instruction\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    response = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    return response[0]['generated_text'][len(prompt):]\n",
    "\n",
    "# Test with sample questions\n",
    "test_cases = [\n",
    "    \"How to raise a staffing SO request?\",\n",
    "    \"What is the process for H1B nomination?\",\n",
    "    \"How can I access Quick SO application?\",\n",
    "    \"Unable to create SO in the system\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Model Testing ===\")\n",
    "for test_case in test_cases:\n",
    "    print(f\"\\nQ: {test_case}\")\n",
    "    print(f\"A: {test_model(test_case)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fb497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance():\n",
    "    \"\"\"Evaluate the model on validation set\"\"\"\n",
    "    print(\"Evaluating model performance...\")\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c541220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_deployment():\n",
    "    \"\"\"Prepare model artifacts for Azure deployment\"\"\"\n",
    "    \n",
    "    deployment_dir = \"./deployment_artifacts\"\n",
    "    \n",
    "    # Create deployment directory\n",
    "    import os\n",
    "    os.makedirs(deployment_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(f\"{deployment_dir}/model\")\n",
    "    tokenizer.save_pretrained(f\"{deployment_dir}/tokenizer\")\n",
    "    \n",
    "    # Create deployment config\n",
    "    deployment_config = {\n",
    "        \"model_name\": \"llama-3.1-8b-corporate-assistant\",\n",
    "        \"model_version\": \"1.0\",\n",
    "        \"framework\": \"transformers\",\n",
    "        \"python_version\": \"3.9\",\n",
    "        \"requirements\": [\n",
    "            \"torch>=2.0.1\",\n",
    "            \"transformers>=4.31.0\",\n",
    "            \"peft>=0.4.0\",\n",
    "            \"bitsandbytes>=0.40.2\",\n",
    "            \"accelerate>=0.21.0\"\n",
    "        ],\n",
    "        \"inference_config\": {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True,\n",
    "            \"repetition_penalty\": 1.1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{deployment_dir}/deployment_config.json\", \"w\") as f:\n",
    "        json.dump(deployment_config, f, indent=2)\n",
    "    \n",
    "    print(f\"Deployment artifacts saved to {deployment_dir}\")\n",
    "    print(\"Ready for Azure deployment!\")\n",
    "    \n",
    "    return deployment_dir\n",
    "\n",
    "# Prepare deployment artifacts\n",
    "deployment_path = prepare_for_deployment()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINE-TUNING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Model saved to: ./llama-3.1-8b-corporate-assistant-final\")\n",
    "print(f\"‚úÖ Deployment artifacts: {deployment_path}\")\n",
    "print(f\"‚úÖ Training metrics saved\")\n",
    "print(f\"‚úÖ Model tested and validated\")\n",
    "print(\"\\nNext steps: Deploy to Azure ML for inferencing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
